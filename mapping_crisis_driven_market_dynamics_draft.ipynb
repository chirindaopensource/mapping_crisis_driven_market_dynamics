{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfg0Q7QduOF1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "# *Mapping Crisis-Driven Market Dynamics: A Transfer Entropy and Kramers–Moyal Approach to Financial Networks* Implementation\n",
        "<br>\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/imports-isort-1674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![Seaborn](https://img.shields.io/badge/seaborn-%233776AB.svg?style=flat&logo=python&logoColor=white)](https://seaborn.pydata.org/)\n",
        "[![NetworkX](https://img.shields.io/badge/NetworkX-blue.svg?style=flat&logo=python&logoColor=white)](https://networkx.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2507.09554-b31b1b.svg)](https://arxiv.org/abs/2507.09554)\n",
        "[![DOI](https://img.shields.io/badge/DOI-10.48550/arXiv.2507.09554-blue)](https://doi.org/10.48550/arXiv.2507.09554)\n",
        "[![Research](https://img.shields.io/badge/Research-Financial%20Networks-green)](https://github.com/chirindaopensource/mapping_crisis_driven_market_dynamics)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Econophysics-blue)](https://github.com/chirindaopensource/mapping_crisis_driven_market_dynamics)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Information%20Theory%20%26%20Stochastic%20Methods-orange)](https://github.com/chirindaopensource/mapping_crisis_driven_market_dynamics)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/mapping_crisis_driven_market_dynamics)\n",
        "<br>\n",
        "\n",
        "**Repository:** https://github.com/chirindaopensource/mapping_crisis_driven_market_dynamics\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade implementation of the research methodology from the 2025 paper entitled **\"Mapping Crisis-Driven Market Dynamics: A Transfer Entropy and Kramers–Moyal Approach to Financial Networks\"** by:\n",
        "\n",
        "*   Pouriya Khalilian\n",
        "*   Amirhossein N. Golestani\n",
        "*   Mohammad Eslamifar\n",
        "*   Mostafa T. Firouzjaee\n",
        "*   Javad T. Firouzjaee\n",
        "\n",
        "The project provides a robust, end-to-end Python pipeline for constructing a multi-layered \"Digital Twin\" of financial market interactions. It moves beyond traditional correlation analysis to map the dynamic, non-linear, and directed relationships between assets, offering a powerful tool for systemic risk analysis, adaptive hedging, and macro-prudential policy assessment.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_master_pipeline](#key-callable-run_master_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Mapping Crisis-Driven Market Dynamics.\" The core of this repository is the iPython Notebook `mapping_crisis_driven_market_dynamics_draft.ipynb`, which contains a comprehensive suite of functions to model financial networks using a dual information-theoretic and stochastic approach.\n",
        "\n",
        "Traditional measures like Pearson correlation are symmetric and linear, failing to capture the complex, directed, and non-linear feedback loops that characterize modern financial markets, especially during crises. This framework addresses these shortcomings by integrating two advanced methodologies:\n",
        "1.  **Transfer Entropy (TE):** A non-parametric measure from information theory that quantifies the directed flow of information between time series.\n",
        "2.  **Kramers-Moyal (KM) Expansion:** A method from stochastic calculus that approximates the underlying deterministic \"drift\" forces governing the system's dynamics.\n",
        "\n",
        "This codebase enables researchers, quantitative analysts, and portfolio managers to:\n",
        "-   Rigorously compute static and dynamic TE and KM network matrices.\n",
        "-   Quantify the intensification of information flow during market crises.\n",
        "-   Identify persistent, stable relationships (e.g., safe-haven effects) and moments of significant structural change (regime shifts).\n",
        "-   Perform advanced robustness and error analysis to validate findings.\n",
        "-   Replicate and extend the results of the original research paper.\n",
        "\n",
        "\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The methodology implemented in this project is a direct translation of the unified framework presented in the source paper. It is designed to overcome the limitations of traditional linear correlation by employing a dual approach rooted in information theory and stochastic calculus. The theoretical pipeline can be understood in four distinct stages:\n",
        "\n",
        "### 1. Foundational Data Transformation\n",
        "\n",
        "The analysis begins with a critical econometric principle: financial asset prices ($P_t$) are generally non-stationary (i.e., they contain unit roots), making them unsuitable for most statistical models. To address this, the pipeline first transforms the raw price series into a stationary log-return series ($r_t$) using the standard formula:\n",
        "\n",
        "$r_t = \\log(P_t) - \\log(P_{t-1})$\n",
        "\n",
        "This transformation yields a series whose statistical properties (like mean and variance) are more stable over time, forming a valid basis for the subsequent analyses. The pipeline empirically verifies this transformation using the Augmented Dickey-Fuller test.\n",
        "\n",
        "### 2. Layer 1: Information-Theoretic Linkages (Transfer Entropy)\n",
        "\n",
        "To map the directed, non-linear flow of information between assets, the framework employs Transfer Entropy (TE). TE measures the reduction in uncertainty about a target asset's future state given knowledge of a source asset's past state, beyond what the target's own past already explains. It is formally defined in the paper's **Equation 2**:\n",
        "\n",
        "$T_{j \\to i} = \\sum_{i_{t+1}, i_t, j_t} P(i_{t+1}, i_t, j_t) \\log_2 \\frac{P(i_{t+1} | i_t, j_t)}{P(i_{t+1} | i_t)}$\n",
        "\n",
        "-   $T_{j \\to i}$ represents the information flowing from asset `j` to asset `i`.\n",
        "-   The measure is inherently **asymmetric** ($T_{j \\to i} \\neq T_{i \\to j}$), allowing us to identify sources and sinks of information flow.\n",
        "-   The calculation requires discretizing the continuous return data into bins to estimate the necessary joint and conditional probability distributions, as outlined conceptually in **Algorithm 1** of the paper's framework.\n",
        "\n",
        "### 3. Layer 2: Stochastic System Dynamics (Kramers-Moyal Expansion)\n",
        "\n",
        "To complement the TE analysis, the framework models the system's evolution using the Kramers-Moyal (KM) expansion. This method describes a stochastic process in terms of its deterministic \"drift\" and stochastic \"diffusion\" components. This implementation focuses on the first KM coefficient—the drift vector $D^{(1)}$—which represents the deterministic forces governing the system's expected movement.\n",
        "\n",
        "The paper makes a crucial simplification by approximating this drift with a linear model:\n",
        "\n",
        "$\\frac{d}{dt}\\mathbf{x}(t) \\approx A\\mathbf{x}(t)$\n",
        "\n",
        "Here, $\\mathbf{x}(t)$ is the vector of asset returns, and $A$ is the $N \\times N$ drift coefficient matrix. The elements $A_{ij}$ represent the signed, linear influence of asset `j`'s current return on the expected change in asset `i`'s return. This matrix is estimated by solving a system of linear equations derived from the moment conditions specified in **Equation 9** of the paper:\n",
        "\n",
        "$\\langle (x_i(t+dt) - x_i(t)) x_k(t) \\rangle = \\sum_{j=1}^{N} A_{ij} \\langle x_j(t) x_k(t) \\rangle$\n",
        "\n",
        "This provides a signed, directed map of the linear relationships, where a negative $A_{ij}$ can be interpreted as a hedging or mean-reverting influence, and a positive $A_{ij}$ suggests co-movement.\n",
        "\n",
        "### 4. Dynamic Analysis via Sliding Window\n",
        "\n",
        "A static analysis of the full time series provides only an average picture of the network. To capture the evolving nature of market dynamics, both the TE and KM analyses are applied within a **sliding window** framework, as described conceptually in **Algorithm 3**. The pipeline moves a window of a fixed size (e.g., 252 days) across the entire dataset with a given step size (e.g., 21 days), re-calculating the TE and KM matrices for each window.\n",
        "\n",
        "This procedure generates a time series of network matrices, transforming the static snapshot into a dynamic \"movie\" of the financial system. This allows for the direct observation of how network structures change over time and, most importantly, how they are reshaped by major market events, which is the central empirical contribution of the source paper.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`mapping_crisis_driven_market_dynamics_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Rigorous Validation:** Comprehensive checks for all input data and configurations.\n",
        "-   **Professional Preprocessing:** A robust pipeline for cleaning financial time series and transforming them into stationary log-returns.\n",
        "-   **Methodologically Pure Calculations:** Precise, numerically stable implementations of the Transfer Entropy and Kramers-Moyal drift matrix calculations.\n",
        "-   **Dynamic Analysis Engine:** A flexible sliding-window framework for time-resolved analysis.\n",
        "-   **Automated Interpretation:** Algorithms to automatically identify persistent network links and detect significant regime shifts.\n",
        "-   **Crisis and Robustness Analysis:** A full suite of tools to quantify crisis impacts and perform sensitivity analysis on key hyperparameters.\n",
        "-   **Error Analysis:** Block bootstrapping to generate confidence intervals for model estimates.\n",
        "-   **Integrated Reporting:** A final function that generates a self-contained, interactive HTML report with embedded tables and figures.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Preparation (Tasks 1-3):** The pipeline ingests raw price data, validates it, preprocesses it into clean log-returns, and confirms the stationarity properties of the resulting series.\n",
        "2.  **Static Network Calculation (Tasks 4-6):** It computes the static (full-period) TE and KM matrices, providing a baseline snapshot of the network.\n",
        "3.  **Dynamic Network Analysis (Tasks 7-8):** It implements the sliding window procedure to generate a time series of network matrices and analyzes these to quantify the average network structure during specific crisis periods versus a normal baseline.\n",
        "4.  **Interpretation and Meta-Analysis (Tasks 10-12):** The pipeline automatically interprets the dynamic results to find persistent links and regime shifts, performs robustness checks across different parameters, and conducts error analysis via bootstrapping.\n",
        "5.  **Reporting (Task 13-14):** All findings are compiled into a comprehensive results bundle and a final, user-selectable HTML or Markdown report.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `mapping_crisis_driven_market_dynamics_draft.ipynb` notebook is structured as a logical pipeline with modular functions for each task:\n",
        "\n",
        "-   **`validate_inputs`**: The initial quality gate for all inputs.\n",
        "-   **`preprocess_price_data`**: The data cleaning and transformation engine.\n",
        "-   **`perform_stationarity_analysis`**: Econometric validation of the data.\n",
        "-   **`generate_descriptive_statistics`**: Initial data characterization.\n",
        "-   **`calculate_transfer_entropy`**: Core information-theoretic calculation.\n",
        "-   **`calculate_kramers_moyal_drift_matrix`**: Core stochastic dynamics calculation.\n",
        "-   **`perform_time_resolved_analysis`**: The dynamic analysis engine.\n",
        "-   **`analyze_crisis_periods`**: Crisis impact quantification.\n",
        "-   **`plot_matrix_heatmap`, `plot_network_graph`**: Visualization utilities.\n",
        "-   **`interpret_dynamic_results`**: Automated interpretation engine.\n",
        "-   **`run_full_analysis_pipeline`**: Orchestrator for a single, complete analysis run.\n",
        "-   **`perform_robustness_analysis`**: Higher-level orchestrator for sensitivity analysis.\n",
        "-   **`perform_error_analysis`**: Higher-level orchestrator for confidence interval estimation.\n",
        "-   **`run_master_pipeline`**: The single, top-level entry point to the entire project.\n",
        "\n",
        "## Key Callable: run_master_pipeline\n",
        "\n",
        "The central function in this project is `run_master_pipeline`. It orchestrates the entire analytical workflow from raw data to final report.\n",
        "\n",
        "```python\n",
        "def run_master_pipeline(\n",
        "    raw_price_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    param_grid: Dict[str, List[Any]],\n",
        "    report_format: str = 'html',\n",
        "    run_robustness_analysis: bool = True,\n",
        "    run_error_analysis: bool = True,\n",
        "    bootstrap_samples: int = 100\n",
        ") -> Tuple[Dict[str, Any], Optional[str]]:\n",
        "    \"\"\"\n",
        "    The master orchestrator for the end-to-end Digital Twin analysis pipeline.\n",
        "    ... (full docstring is in the notebook)\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies as listed in `requirements.txt`: `pandas`, `numpy`, `scipy`, `matplotlib`, `seaborn`, `networkx`, `statsmodels`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/mapping_crisis_driven_market_dynamics.git\n",
        "    cd mapping_crisis_driven_market_dynamics\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies from `requirements.txt`:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The primary input is a `pandas.DataFrame` with a `DatetimeIndex` and columns containing the daily closing prices of the assets to be analyzed. The index should consist of business days.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "                  Nasdaq  Crude-oil      Gold  US-dollar\n",
        "Date\n",
        "2014-08-11  4401.330078  98.080002  1310.300049  81.489998\n",
        "2014-08-12  4434.129883  97.370003  1310.500000  81.620003\n",
        "2014-08-13  4456.020020  97.570000  1314.599976  81.580002\n",
        "...                 ...        ...          ...        ...\n",
        "```\n",
        "\n",
        "## Usage\n",
        "\n",
        "The entire pipeline is executed through the `run_master_pipeline` function. The user must provide the raw price data, a study configuration dictionary, and a parameter grid for robustness checks.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load your data\n",
        "# raw_price_df = pd.read_csv(\"your_data.csv\", index_col=0, parse_dates=True)\n",
        "# For this example, we create synthetic data.\n",
        "date_rng = pd.date_range(start='2014-08-01', end='2024-09-30', freq='B')\n",
        "price_data = {asset: 100 * np.exp(np.cumsum(np.random.randn(len(date_rng)) * 0.01)) for asset in ['Nasdaq', 'Crude-oil', 'Gold', 'US-dollar']}\n",
        "raw_price_df = pd.DataFrame(price_data, index=date_rng)\n",
        "\n",
        "# 2. Define your configurations (see notebook for full example)\n",
        "study_config = { ... } # As defined in the notebook\n",
        "param_grid = { ... }   # As defined in the notebook\n",
        "\n",
        "# 3. Run the master pipeline\n",
        "# from mapping_crisis_driven_market_dynamics_draft import run_master_pipeline\n",
        "master_results, html_report = run_master_pipeline(\n",
        "    raw_price_df=raw_price_df,\n",
        "    study_config=study_config,\n",
        "    param_grid=param_grid,\n",
        "    report_format='html'\n",
        ")\n",
        "\n",
        "# 4. Save the report and explore results\n",
        "with open(\"analysis_report.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_report)\n",
        "\n",
        "# Programmatically access results\n",
        "robust_links = master_results['robustness_analysis']['persistent_links']['transfer_entropy']\n",
        "print(robust_links.head())\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_master_pipeline` function returns a tuple: `(master_results, report_string)`.\n",
        "\n",
        "-   `master_results`: A deeply nested dictionary containing all data artifacts. Top-level keys include:\n",
        "    -   `main_analysis`: Results from the baseline run (processed data, static matrices, dynamic results, interpretations, figures).\n",
        "    -   `robustness_analysis`: Results from the sensitivity analysis, including robustness scores for key findings.\n",
        "    -   `error_analysis`: Results from the bootstrapping, including confidence intervals for static estimates.\n",
        "-   `report_string`: A string containing the full source code of the generated HTML or Markdown report.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "mapping_crisis_driven_market_dynamics/\n",
        "│\n",
        "├── mapping_crisis_driven_market_dynamics_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                                   # Python package dependencies\n",
        "├── LICENSE                                            # MIT license file\n",
        "└── README.md                                          # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `study_config` and `param_grid` dictionaries passed to `run_master_pipeline`. Users can easily modify:\n",
        "-   The `date_range` and `crisis_periods` to analyze different time frames.\n",
        "-   The `discretization_bins` and `window_size_days` to test different model specifications.\n",
        "-   All `visualization_params` to change the aesthetic of plots and graphs.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{khalilian2025mapping,\n",
        "  title={Mapping Crisis-Driven Market Dynamics: A Transfer Entropy and Kramers--Moyal Approach to Financial Networks},\n",
        "  author={Khalilian, Pouriya and Golestani, Amirhossein N and Eslamifar, Mohammad and Firouzjaee, Mostafa T and Firouzjaee, Javad T},\n",
        "  journal={arXiv preprint arXiv:2507.09554},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of the Transfer Entropy and Kramers-Moyal Framework for Financial Networks.\n",
        "GitHub repository: https://github.com/chirindaopensource/mapping_crisis_driven_market_dynamics\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Pouriya Khalilian, Amirhossein N. Golestani, Mohammad Eslamifar, Mostafa T. Firouzjaee, and Javad T. Firouzjaee for the novel analytical framework.\n",
        "-   Thanks to the developers of the `pandas`, `numpy`, `scipy`, `matplotlib`, `seaborn`, `networkx`, `statsmodels`, and `tqdm` libraries, which are the foundational pillars of this analytical pipeline.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated to document the code and methodology contained within `mapping_crisis_driven_market_dynamics_draft.ipynb` and follows best practices for open-source research software.*"
      ],
      "metadata": {
        "id": "yV1yHtfj3KmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "**Title:** \"*Mapping Crisis-Driven Market Dynamics: A Transfer Entropy and KramersMoyal Approach to Financial Networks*\"\n",
        "\n",
        "**Link:** https://arxiv.org/abs/2507.09554\n",
        "\n",
        "**Authors:** Pouriya Khalilian, Amirhossein N. Golestani, Mohammad Eslamifar, Mostafa T. Firouzjaee, Javad T. Firouzjaee\n",
        "\n",
        "**E-Journal Submission Date:** 13th of July 2025\n",
        "\n",
        "**Abstract:**\n",
        "\n",
        "Financial markets are dynamic, interconnected systems where local shocks can trigger widespread instability, challenging portfolio managers and policymakers. Traditional correlation analysis often miss the directionality and temporal dynamics of information flow. To address this, we present a unified framework integrating Transfer Entropy (TE) and the N-dimensional KramersMoyal (KM) expansion to map static and time-resolved coupling among four major indices: Nasdaq Composite (^IXIC), WTI crude oil (WTI), gold (GC=F), and the US Dollar Index (this http URL). TE captures directional information flow. KM models non-linear stochastic dynamics, revealing interactions often overlooked by linear methods. Using daily data from August 11, 2014, to September 8, 2024, we compute returns, confirm non-stationary using a conduct sliding-window TE and KM analyses. We find that during the COVID-19 pandemic (March-June 2020) and the Russia-Ukraine crisis (Feb-Apr 2022), average TE increases by 35% and 28%, respectively, indicating heightened directional flow. Drift coefficients highlight gold-dollar interactions as a persistent safe-haven channel, while oil-equity linkages show regime shifts, weakening under stress and rebounding quickly. Our results expose the shortcomings of linear measures and underscore the value of combining information-theoretic and stochastic drift methods. This approach offers actionable insights for adaptive hedging and informs macro-prudential policy by revealing the evolving architecture of systemic risk."
      ],
      "metadata": {
        "id": "ZMcZp0hnub_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "#### **Step 1: The Core Problem and the Paper's Proposed Solution**\n",
        "\n",
        "First, let's understand the fundamental problem the authors are trying to solve. For decades, we've used the **correlation coefficient** to understand how financial assets move together. However, this tool is fundamentally limited:\n",
        "\n",
        "1.  **It's Symmetric:** The correlation between Gold and the US Dollar is a single number. It doesn't tell you if Gold is driving the Dollar or if the Dollar is driving Gold. It lacks **directionality**.\n",
        "2.  **It's Linear:** It only captures linear relationships. It completely misses complex, non-linear interactions, which we know are rampant in financial markets, especially during crises.\n",
        "3.  **It's Static:** A single correlation number for a whole period hides how relationships evolve over time.\n",
        "\n",
        "The authors propose a more sophisticated, two-pronged approach to overcome these limitations. They aim to create a \"unified framework\" by combining:\n",
        "\n",
        "*   **Transfer Entropy (TE):** An information-theoretic tool to capture **directed, non-linear information flow**.\n",
        "*   **Kramers–Moyal (KM) Expansion:** A technique from stochastic calculus to derive the underlying **dynamic equations (the \"rules of the game\")** that govern the assets' movements.\n",
        "\n",
        "The goal is to not just see *that* assets are connected, but to understand *how* they influence each other, in which direction, and how these rules change, particularly during market stress.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 2: Deconstructing the Methodology**\n",
        "\n",
        "This is the heart of the paper. Let's look at the two tools they use.\n",
        "\n",
        "**Part A: Transfer Entropy (TE)**\n",
        "Think of TE as a sophisticated measure of Granger causality. In simple terms, TE from asset X to asset Y measures the reduction in uncertainty about the future of Y when you know the past of X, over and above what you already knew from the past of Y alone.\n",
        "\n",
        "*   **What it gives you:** A directed measure (`T_XY` is not necessarily equal to `T_YX`). It's model-free and can capture non-linear relationships.\n",
        "*   **How they use it:** They calculate TE between all pairs of their four indices (Nasdaq, Oil, Gold, US Dollar) and use a sliding window to see how these information flows evolve over time. This allows them to create dynamic, directed networks.\n",
        "\n",
        "**Part B: The N-dimensional Kramers–Moyal (KM) Expansion**\n",
        "This is the more complex part. Imagine a stock price moving randomly. The KM expansion is a formal way to extract the underlying \"forces\" driving that movement directly from the time-series data. Any stochastic process can be described by its KM coefficients. The two most important are:\n",
        "\n",
        "*   **Drift Coefficient (D⁽¹⁾):** The deterministic force. This is the \"push\" or \"pull\" on the asset's price at any given moment. It tells you the expected direction of the next move.\n",
        "*   **Diffusion Coefficient (D⁽²⁾):** The stochastic force. This is the magnitude of the random noise or volatility.\n",
        "\n",
        "The authors make a **critical simplification**: They focus only on the drift term (D⁽¹⁾) and, more importantly, they *assume it has a linear form*. They model the system's dynamics as a set of linear differential equations: `dx/dt = Ax(t)`.\n",
        "\n",
        "*   **What this gives them:** The matrix `A` becomes a network of influences. The element `A_ij` represents the linear push that asset `j` exerts on asset `i`. A negative diagonal element `A_ii` implies mean-reversion (stability), while a positive one would imply explosive behavior.\n",
        "*   **How they use it:** They estimate this `A` matrix for the whole period and also within a sliding window to see how these deterministic influences change over time.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 3: Key Empirical Findings**\n",
        "\n",
        "The authors apply these methods to daily data for the Nasdaq, WTI crude oil, gold, and the US Dollar Index from 2014 to 2024. Their results are quite intuitive.\n",
        "\n",
        "1.  **Crises Intensify Information Flow:** The Transfer Entropy analysis (Figures 6 & 8) clearly shows that the total information flow in the network spikes dramatically during the COVID-19 pandemic (starting 2020) and the Russia-Ukraine conflict (starting 2022). This confirms our intuition that in times of panic, markets become more tightly coupled and information (or noise) transmits more rapidly.\n",
        "2.  **Gold's Persistent Safe-Haven Role:** The Kramers-Moyal drift analysis (Figure 11) reveals a consistent, stable interaction between gold and the US dollar. This provides a dynamic, model-based confirmation of gold's role as a safe-haven asset that tends to move in opposition to the dollar.\n",
        "3.  **Fragile Oil-Equity Link:** In contrast, the relationship between oil and the Nasdaq (a proxy for equities/tech sector) is shown to be less stable. The linkage appears to weaken or change character during crises and then re-form afterward, highlighting a \"regime-dependent\" dynamic that simple correlation would miss.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 4: The Paper's Main Contribution**\n",
        "\n",
        "The primary contribution here is the **synthesis of the two methods**.\n",
        "\n",
        "*   TE tells you *if* and *in which direction* information is flowing.\n",
        "*   The KM drift analysis provides a *mechanistic model* of that flow.\n",
        "\n",
        "For example, TE might show a strong flow from the US Dollar to Gold. The KM analysis then adds color to this, showing that the drift coefficient is negative, meaning a stronger dollar exerts a downward \"push\" on the price of gold. Combining these gives a richer story than either one could tell alone. It's a step up from simply drawing lines on a network graph to trying to understand the physics of the system.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 5: A Professor's Critique and Avenues for Future Work**\n",
        "\n",
        "While the paper is a valuable proof-of-concept, if this were a student's dissertation proposal, I would raise a few critical points:\n",
        "\n",
        "*   **The Linearity Assumption is a Major Caveat:** The biggest weakness is simplifying the Kramers-Moyal drift term to a linear model (`Ax(t)`). The whole point of using advanced methods like KM is to escape the confines of linearity. By making this assumption, their drift analysis becomes conceptually very similar to a standard Vector Autoregressive (VAR) model. A key area for future work would be to estimate the *non-linear* drift functions, which is computationally challenging but would unlock the true power of the KM approach.\n",
        "*   **The \"N-dimensional\" Claim:** The paper calls this an \"N-dimensional\" KM approach. However, true non-parametric estimation of drift and diffusion in N-dimensions suffers heavily from the \"curse of dimensionality.\" The linear assumption is what makes their N=4 case tractable. The methodology for estimating the matrix `A` from the data (Eq. 10) is essentially solving a system of linear equations, which sidesteps the harder non-parametric problem.\n",
        "*   **Parameter Sensitivity:** The robustness of both TE and KM estimations depends heavily on parameter choices (e.g., embedding delays for TE, the time-step `dt` for KM, the sliding window size). The paper does not discuss the sensitivity of its results to these choices, which is essential for validating the findings.\n",
        "*   **Stationarity:** The use of a sliding window is a pragmatic way to handle the non-stationarity of financial data. However, it implicitly assumes that the process is \"locally stationary\" within each window. This is a necessary evil in financial econometrics, but it's a tension worth acknowledging.\n",
        "\n",
        "**In conclusion,** Khalilian et al. present a compelling and well-motivated study. They successfully demonstrate that combining information theory and stochastic modeling provides a more nuanced and dynamic picture of financial networks than traditional methods. The paper serves as an excellent illustration of how these advanced tools can be applied. Its main limitation—the linearity assumption in the KM analysis—is also its greatest opportunity for future research to build upon."
      ],
      "metadata": {
        "id": "vHhr-_yV1FTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "Au6njmUkr6YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================================================================\n",
        "#\n",
        "#  Python Implementation of \"Mapping Crisis-Driven Market Dynamics: A Transfer Entropy and KramersMoyal Approach to Financial Networks\"\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Mapping Crisis-Driven Market Dynamics:\n",
        "#  A Transfer Entropy and Kramers-Moyal Approach to Financial Networks\".\n",
        "#\n",
        "#  It includes a full suite of functions for:\n",
        "#   - Rigorous data validation and preprocessing.\n",
        "#   - Core calculation of Transfer Entropy and Kramers-Moyal drift coefficients.\n",
        "#   - Time-resolved dynamic analysis via a sliding window procedure.\n",
        "#   - Automated interpretation, crisis analysis, and network visualization.\n",
        "#   - Advanced meta-analysis including robustness and error analysis.\n",
        "#\n",
        "#  The final output is a multi-layered \"Digital Twin\" of market interactions,\n",
        "#  designed for advanced risk management and systemic analysis.\n",
        "#\n",
        "# ==========================================================================================================================================\n",
        "\n",
        "# Standard Library Imports\n",
        "import base64\n",
        "import io\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "from typing import (Any, Dict, Generator, List, Optional, Tuple, Union)\n",
        "\n",
        "# Third-Party Library Imports\n",
        "# Core Data Handling and Numerical Computation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "from numpy.linalg import LinAlgError\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "# Network Analysis\n",
        "import networkx as nx\n",
        "\n",
        "# Econometric and Statistical Modeling\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Utility for Progress Monitoring\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "QUjCl16Hr-Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "IcSjOOPFsBWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Inputs, Processes and Outputs (IPO) Analysis of Key Callables\n",
        "\n",
        "### **Comprehensive Callable-by-Callable Analysis**\n",
        "\n",
        "#### **Module 1: Input Validation**\n",
        "\n",
        "**1.1. `_validate_dataframe`**\n",
        "*   **Inputs:** `df` (a `pandas.DataFrame`).\n",
        "*   **Processes:** Performs three specific checks on the input DataFrame: (1) verifies that the index is a `pandas.DatetimeIndex` and is monotonically increasing; (2) verifies that all data columns are of a numeric data type; (3) verifies that the DataFrame contains no `NaN` (missing) values.\n",
        "*   **Outputs:** A `list` of `str`, where each string is a descriptive error message. An empty list signifies successful validation.\n",
        "*   **Role in Research Pipeline:** This is a granular **Data Integrity Check**. It ensures the foundational data structure meets the absolute minimum requirements for time series analysis before any further processing.\n",
        "\n",
        "**1.2. `_validate_study_config`**\n",
        "*   **Inputs:** `config` (a `dict`).\n",
        "*   **Processes:** (1) Recursively validates the input dictionary against a predefined schema to ensure all required keys and nested structures are present. (2) For all date-string values, it attempts to parse them into `pandas.Timestamp` objects to validate their format. (3) It performs logical checks, such as ensuring start dates precede end dates and that crisis periods fall within the main study date range. (4) It validates the data types and value ranges of key numerical parameters (e.g., `discretization_bins` must be a positive integer).\n",
        "*   **Outputs:** A `tuple` containing: (1) a deep copy of the input `config` with date strings converted to `pd.Timestamp` objects, and (2) a `list` of `str` error messages.\n",
        "*   **Role in Research Pipeline:** This is the **Parameter Validation** unit. It ensures that all user-defined parameters for the study are present, correctly formatted, and logically consistent.\n",
        "\n",
        "**1.3. `_cross_validate_inputs`**\n",
        "*   **Inputs:** `df` (a `pandas.DataFrame`), `config` (a `dict` where dates are `pd.Timestamp` objects).\n",
        "*   **Processes:** Performs a temporal consistency check by comparing the minimum and maximum dates in the `df.index` with the `start` and `end` dates specified in the `config`.\n",
        "*   **Outputs:** A `list` of `str` error messages if the DataFrame's date range does not fully encompass the required study range.\n",
        "*   **Role in Research Pipeline:** This is the **Data-Parameter Compatibility Check**. It confirms that the provided data is sufficient to actually perform the analysis requested in the parameters.\n",
        "\n",
        "**1.4. `validate_inputs`**\n",
        "*   **Inputs:** `df` (a `pandas.DataFrame`), `config` (a `dict`).\n",
        "*   **Processes:** Orchestrates the validation by calling `_validate_dataframe`, `_validate_study_config`, and `_cross_validate_inputs` in sequence. It aggregates all potential error messages from these helpers. If any errors are found, it raises a single, comprehensive `ValueError`.\n",
        "*   **Outputs:** The validated and type-converted configuration `dict`.\n",
        "*   **Role in Research Pipeline:** This is the master **Gatekeeper and Initializer** for the entire pipeline, ensuring all inputs are sound before computation begins.\n",
        "\n",
        "#### **Module 2: Data Preprocessing**\n",
        "\n",
        "**2.1. `preprocess_price_data`**\n",
        "*   **Inputs:** `df` (a raw, validated `pandas.DataFrame`), `validated_config` (a `dict`).\n",
        "*   **Processes:** Executes a strict, four-step data transformation pipeline: (1) **Filtering:** Slices the DataFrame to the study's date range. (2) **Cleaning:** Replaces infinities, then applies forward-fill and backward-fill to handle missing values. (3) **Transformation:** Checks for non-positive prices, then calculates log-returns using the formula $r_t = \\log(P_t) - \\log(P_{t-1})$. (4) **Finalization:** Drops any remaining `NaN` rows to ensure a perfectly aligned dataset.\n",
        "*   **Outputs:** A `tuple` containing: (1) a clean, stationary `pandas.DataFrame` of log-returns, and (2) a `dict` of metadata documenting the process.\n",
        "*   **Role in Research Pipeline:** This function is the **Data Transformation Engine**, converting raw, non-stationary price data into the clean, stationary log-return series required by the core analytical models.\n",
        "\n",
        "#### **Module 3: Econometric Diagnostics**\n",
        "\n",
        "**3.1. `_run_adf_test`**\n",
        "*   **Inputs:** `series` (a `pandas.Series`), `significance_level` (a `float`).\n",
        "*   **Processes:** Executes the Augmented Dickey-Fuller (ADF) test on the input series using `statsmodels.tsa.stattools.adfuller`. It uses a constant and trend (`ct`) regression and automatic lag selection (`AIC`). It interprets the resulting p-value to determine if the series is stationary.\n",
        "*   **Outputs:** A `dict` containing the ADF statistic, p-value, lags used, and a boolean `is_stationary` flag with a human-readable interpretation.\n",
        "*   **Role in Research Pipeline:** This is a granular **Statistical Test Unit**. It performs a single, well-configured stationarity test.\n",
        "\n",
        "**3.2. `perform_stationarity_analysis`**\n",
        "*   **Inputs:** `price_df` (a `pandas.DataFrame`), `log_return_df` (a `pandas.DataFrame`).\n",
        "*   **Processes:** For each asset, it calls `_run_adf_test` on both its price series and its log-return series. It then compiles these paired results into a single summary table.\n",
        "*   **Outputs:** A multi-indexed `pandas.DataFrame` that presents the stationarity test results for prices and returns side-by-side for each asset.\n",
        "*   **Role in Research Pipeline:** This function performs the **Econometric Validation** of the data transformation, empirically confirming that the log-return series are stationary and suitable for analysis, while the original price series are not.\n",
        "\n",
        "#### **Module 4: Core Analysis**\n",
        "\n",
        "**4.1. `_discretize_data`**\n",
        "*   **Inputs:** `log_return_df` (a `pandas.DataFrame`), `num_bins` (an `int`).\n",
        "*   **Processes:** For each column in the input DataFrame, it applies `pandas.qcut` to perform equal-frequency (quantile) binning, converting the continuous data into discrete integer labels from 0 to `num_bins-1`.\n",
        "*   **Outputs:** A `tuple` containing: (1) a `pandas.DataFrame` of the same shape with integer bin labels, and (2) a `dict` mapping each asset to its calculated bin edges.\n",
        "*   **Role in Research Pipeline:** This is the **Data Discretization** step, a necessary prerequisite for any analysis based on probability mass functions, specifically Transfer Entropy.\n",
        "\n",
        "**4.2. `calculate_transfer_entropy`**\n",
        "*   **Inputs:** `log_return_df` (a `pandas.DataFrame`), `validated_config` (a `dict`).\n",
        "*   **Processes:** Implements the direct calculation of Transfer Entropy as defined by **Equation 2** of the source paper. The process involves: (1) discretizing the data using `_discretize_data`; (2) for each pair of assets (j->i), constructing the 3D joint probability distribution $P(i_{t+1}, i_t, j_t)$; (3) calculating the required marginal and conditional probabilities with numerically stable division; (4) computing the log-ratio of the conditional probabilities; (5) calculating the final TE value as the expectation of this log-ratio over the joint distribution.\n",
        "*   **Outputs:** An $N \\times N$ `pandas.DataFrame` where the element at `[row_i, col_j]` is the Transfer Entropy from asset `j` to asset `i`.\n",
        "*   **Role in Research Pipeline:** This is a core analytical function that implements the **Information-Theoretic Network Mapping**, quantifying non-linear, directed information flows.\n",
        "\n",
        "**4.3. `calculate_kramers_moyal_drift_matrix`**\n",
        "*   **Inputs:** `log_return_df` (a `pandas.DataFrame`).\n",
        "*   **Processes:** Implements the estimation of the first Kramers-Moyal coefficient (the drift matrix). It (1) calculates the one-step increments of the log-return series; (2) computes the covariance matrix ($C_{xx}$) and the time-lagged cross-covariance matrix ($C_{yx}$); (3) checks the condition number of $C_{xx}$ for stability; (4) solves the linear matrix equation $A = C_{yx} C_{xx}^{-1}$ to find the drift matrix $A$. This solves the system derived from **Equation 9**.\n",
        "*   **Outputs:** An $N \\times N$ `pandas.DataFrame` representing the estimated linear drift matrix $A$.\n",
        "*   **Role in Research Pipeline:** This is the second core analytical function, implementing the **Stochastic Dynamics Network Mapping**, providing a linear approximation of the directed forces within the system.\n",
        "\n",
        "#### **Module 5: Dynamic Analysis and Interpretation**\n",
        "\n",
        "**5.1. `perform_time_resolved_analysis`**\n",
        "*   **Inputs:** `log_return_df` (a `pandas.DataFrame`), `validated_config` (a `dict`).\n",
        "*   **Processes:** Implements the **Algorithm 3** sliding window procedure. It (1) generates sequential, overlapping windows of the data; (2) iteratively calls `calculate_transfer_entropy` and `calculate_kramers_moyal_drift_matrix` on each window; (3) robustly handles errors in individual windows; (4) stores the resulting sequence of matrices in dictionaries indexed by the window end-date.\n",
        "*   **Outputs:** A `dict` containing the time series of TE and KM matrices.\n",
        "*   **Role in Research Pipeline:** This function provides the **Dynamic Evolution** of the network, allowing for the study of how market structure changes over time.\n",
        "\n",
        "**5.2. `analyze_crisis_periods`**\n",
        "*   **Inputs:** `time_resolved_results` (a `dict`), `validated_config` (a `dict`).\n",
        "*   **Processes:** (1) Partitions the time-resolved matrices into \"crisis\" and \"baseline\" sets based on dates in the configuration. (2) Aggregates the matrices in each set by computing the element-wise mean. (3) Calculates the percentage change of the average crisis matrix relative to the baseline.\n",
        "*   **Outputs:** A nested `dict` containing the average matrices and percentage change matrices for each crisis period.\n",
        "*   **Role in Research Pipeline:** This function performs the **Crisis Impact Quantification**, providing the evidence for claims about how crises intensify information flow.\n",
        "\n",
        "**5.3. `_unstack_results_to_dataframe`**\n",
        "*   **Inputs:** `time_resolved_results` (a `dict`), `analysis_type` (a `str`).\n",
        "*   **Processes:** Transforms the 3D data structure (dictionary of matrices over time) into a 2D `pandas.DataFrame`. It \"melts\" the data so that the index is time and the columns are a `MultiIndex` representing each unique network link `(target, source)`.\n",
        "*   **Outputs:** A 2D `pandas.DataFrame` where each column is the time series of a single network link's strength.\n",
        "*   **Role in Research Pipeline:** This is a crucial **Data Reshaping Utility** that prepares the dynamic results for standard time series analysis.\n",
        "\n",
        "**5.4. `interpret_dynamic_results`**\n",
        "*   **Inputs:** `time_resolved_results` (a `dict`) and several threshold parameters.\n",
        "*   **Processes:** Performs a two-part automated interpretation. (1) **Persistence Analysis:** It identifies links that have a high average strength (magnitude) and are consistently strong over time (consistency). (2) **Regime Shift Detection:** It uses a rolling z-score method to screen for dates where a link's short-term behavior deviates significantly from its long-term norm.\n",
        "*   **Outputs:** A `dict` containing DataFrames of persistent links and a dictionary of detected regime shift events.\n",
        "*   **Role in Research Pipeline:** This is the **Automated Interpretation Engine**, which sifts through the dynamic results to flag the most stable relationships and the most significant moments of change.\n",
        "\n",
        "#### **Module 6: Visualization and Reporting**\n",
        "\n",
        "**6.1. `plot_matrix_heatmap`**\n",
        "*   **Inputs:** `matrix_df` (a `pandas.DataFrame`), `title` (a `str`), `validated_config` (a `dict`).\n",
        "*   **Processes:** Uses `seaborn.heatmap` to create a graphical color-coded representation of the input matrix, with intelligent colormap selection and annotations.\n",
        "*   **Outputs:** A `matplotlib.figure.Figure` object.\n",
        "*   **Role in Research Pipeline:** A **Visualization Tool** for displaying matrix-based results, analogous to the heatmap in Figure 2 of the paper.\n",
        "\n",
        "**6.2. `plot_network_graph`**\n",
        "*   **Inputs:** `matrix_df` (a `pandas.DataFrame`), `title` (a `str`), `validated_config` (a `dict`).\n",
        "*   **Processes:** Uses `networkx` to convert the matrix into a directed graph object. It then draws this graph, using data-driven visual properties (edge width, color, node size) and filters edges by a percentile threshold to improve clarity.\n",
        "*   **Outputs:** A `matplotlib.figure.Figure` object.\n",
        "*   **Role in Research Pipeline:** A **Visualization Tool** for displaying network structures, analogous to the network diagram in Figure 2 of the paper.\n",
        "\n",
        "**6.3. `_figure_to_base64_str`**\n",
        "*   **Inputs:** `fig` (a `matplotlib.figure.Figure` object).\n",
        "*   **Processes:** Renders the figure to an in-memory binary buffer, encodes the binary data to Base64, and formats it as a self-contained data URI string.\n",
        "*   **Outputs:** A `str` containing the data URI.\n",
        "*   **Role in Research Pipeline:** A **Reporting Utility** that enables the embedding of plots directly into HTML documents.\n",
        "\n",
        "**6.4. `_generate_html_report`**\n",
        "*   **Inputs:** `master_results` (a `dict`).\n",
        "*   **Processes:** Constructs a complete, self-contained HTML document as a string. It converts result DataFrames to styled HTML tables using `.to_html()` and embeds figures by calling `_figure_to_base64_str` and inserting the resulting data URI into `<img>` tags. It uses `<details>` tags for interactivity.\n",
        "*   **Outputs:** A `str` containing the full HTML source of the report.\n",
        "*   **Role in Research Pipeline:** The **Dynamic Report Generator**, creating a portable, interactive, and visually rich summary of the entire analysis.\n",
        "\n",
        "#### **Module 7: Master Orchestration**\n",
        "\n",
        "**7.1. `run_full_analysis_pipeline`**\n",
        "*   **Inputs:** `raw_price_df` (a `pandas.DataFrame`), `study_config` (a `dict`).\n",
        "*   **Processes:** Orchestrates the execution of Tasks 1 through 10 in a single, sequential workflow. It manages the flow of data artifacts between the component functions.\n",
        "*   **Outputs:** A comprehensive `dict` containing all results from the main analysis pipeline.\n",
        "*   **Role in Research Pipeline:** This is the **Core Analysis Orchestrator**, representing a single, complete run of the paper's methodology.\n",
        "\n",
        "**7.2. `perform_robustness_analysis`**\n",
        "*   **Inputs:** `raw_price_df` (a `pandas.DataFrame`), `base_config` (a `dict`), `param_grid` (a `dict`).\n",
        "*   **Processes:** (1) Generates all possible hyperparameter combinations from the `param_grid`. (2) For each combination, it calls `run_full_analysis_pipeline`. (3) It aggregates the key findings (persistent links and regime shifts) from all runs. (4) It calculates a \"Robustness Score\" for each finding, representing the percentage of runs in which it was detected.\n",
        "*   **Outputs:** A `dict` containing DataFrames that summarize the stability of the key findings.\n",
        "*   **Role in Research Pipeline:** This is the **Sensitivity Analysis Engine**, which stress-tests the conclusions of the study against changes in its underlying assumptions.\n",
        "\n",
        "**7.3. `perform_error_analysis`**\n",
        "*   **Inputs:** `log_return_df` (a `pandas.DataFrame`), `validated_config` (a `dict`), `num_samples` (an `int`).\n",
        "*   **Processes:** (1) Implements block bootstrapping to generate numerous resampled versions of the time series data. (2) For each resampled dataset, it re-calculates the static TE and KM matrices. (3) It uses the resulting distribution of matrix coefficients to compute percentile-based confidence intervals for each link.\n",
        "*   **Outputs:** A `dict` containing the point estimates and the lower/upper confidence interval matrices.\n",
        "*   **Role in Research Pipeline:** This is the **Statistical Inference Engine**, which quantifies the uncertainty of the model's estimates due to sampling noise.\n",
        "\n",
        "**7.4. `run_master_pipeline`**\n",
        "*   **Inputs:** `raw_price_df`, `study_config`, `param_grid`, and several boolean flags.\n",
        "*   **Processes:** The ultimate orchestrator. It (1) calls `run_full_analysis_pipeline` to get the main results; (2) conditionally calls `perform_robustness_analysis` and `perform_error_analysis` based on user flags; (3) compiles all results into a single master bundle; (4) conditionally calls `_generate_html_report` (or `_generate_report_string`) to create a final summary document.\n",
        "*   **Outputs:** A `tuple` containing: (1) the master results `dict`, and (2) the generated report `str` (or `None`).\n",
        "*   **Role in Research Pipeline:** This is the **Master Conductor**, the single entry point that encapsulates the entire research project from raw data to final, documented, and stress-tested conclusions.\n",
        "\n",
        "### Usage Illustration\n",
        "### **Example: Executing the Digital Twin Master Pipeline**\n",
        "\n",
        "This example demonstrates a complete, end-to-end workflow for analyzing the relationships between four major financial assets, as described in the source paper. We will:\n",
        "1.  Load sample data.\n",
        "2.  Define the comprehensive study configuration.\n",
        "3.  Define the parameter grid for robustness checks.\n",
        "4.  Execute the master pipeline function.\n",
        "5.  Process and display the outputs.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 1: Setup and Data Loading**\n",
        "\n",
        "First, we import the necessary libraries and our master pipeline function. For this example, we will create a synthetic dataset that mimics the structure of real financial data. In a real-world scenario, this data would be loaded from a file or downloaded via an API.\n",
        "\n",
        "```python\n",
        "# Import the master pipeline function and necessary libraries\n",
        "# In a real package, this would be: from digital_twin_pipeline import run_master_pipeline\n",
        "# For this example, we assume all functions are in the same script.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Create Synthetic Raw Price Data ---\n",
        "# In a real application, you would load your data here, e.g., from a CSV:\n",
        "# raw_price_df = pd.read_csv(\"asset_prices.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "# For this example, we generate synthetic data that resembles the assets in the paper.\n",
        "date_rng = pd.date_range(start='2014-08-01', end='2024-09-30', freq='B')\n",
        "num_days = len(date_rng)\n",
        "assets = ['Nasdaq', 'Crude-oil', 'Gold', 'US-dollar']\n",
        "\n",
        "# Generate random walks to simulate price series\n",
        "np.random.seed(42)\n",
        "price_data = {\n",
        "    asset: 100 * np.exp(np.cumsum(np.random.randn(num_days) * 0.01))\n",
        "    for asset in assets\n",
        "}\n",
        "raw_price_df = pd.DataFrame(price_data, index=date_rng)\n",
        "\n",
        "print(\"Sample Raw Price Data:\")\n",
        "print(raw_price_df.head())\n",
        "```\n",
        "**Discursive Text:** The code above sets up our environment. We create a `pandas.DataFrame` named `raw_price_df`. The index is a `DatetimeIndex` of business days, and the columns represent the different financial assets we want to analyze. This structure is the required input format for our pipeline.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 2: Defining the Study Configuration**\n",
        "\n",
        "Next, we define the two critical configuration dictionaries. These parameters control every aspect of the analysis, from the date ranges to the visualization styles.\n",
        "\n",
        "```python\n",
        "# --- Define the Main Study Configuration ---\n",
        "# This dictionary controls the primary analysis run.\n",
        "study_config = {\n",
        "    'data_params': {\n",
        "        'date_range': {'start': '2014-08-11', 'end': '2024-09-08'},\n",
        "        'crisis_periods': {\n",
        "            'covid_19': {\n",
        "                'start': '2020-03-01',\n",
        "                'end': '2020-06-30',\n",
        "                'label': 'COVID-19 Pandemic'\n",
        "            },\n",
        "            'ukraine_war': {\n",
        "                'start': '2022-02-24',\n",
        "                'end': '2022-05-31',\n",
        "                'label': 'Ukraine War'\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'analysis_params': {\n",
        "        'information_theory': {\n",
        "            'discretization_bins': 10,\n",
        "            'discretization_strategy': 'equal_frequency'\n",
        "        },\n",
        "        'sliding_window': {\n",
        "            'window_size_days': 252, # Approx. 1 trading year\n",
        "            'step_size_days': 21     # Approx. 1 trading month\n",
        "        },\n",
        "        'kramers_moyal': {\n",
        "            'model_assumption': 'linear_drift'\n",
        "        }\n",
        "    },\n",
        "    'visualization_params': {\n",
        "        'general': {'figure_dpi': 300, 'font_size': 10},\n",
        "        'network_graphs': {\n",
        "            'layout_function': 'circular_layout',\n",
        "            'node_size': 3000,\n",
        "            'node_color': 'skyblue',\n",
        "            'edge_width_multiplier': 5.0,\n",
        "            'arrow_size': 20\n",
        "        },\n",
        "        'heatmaps': {\n",
        "            'colormap_diverging': 'RdBu_r',\n",
        "            'colormap_sequential': 'viridis',\n",
        "            'crisis_line_color': 'black',\n",
        "            'crisis_line_style': '--'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Define the Parameter Grid for Robustness Checks ---\n",
        "# This dictionary specifies which parameters to vary and what values to test.\n",
        "param_grid = {\n",
        "    'analysis_params.information_theory.discretization_bins': [8, 10, 12],\n",
        "    'analysis_params.sliding_window.window_size_days': [189, 252] # Approx. 9 months and 12 months\n",
        "}\n",
        "```\n",
        "**Discursive Text:** We have created two dictionaries. `study_config` defines our baseline analysis. `param_grid` tells the robustness analysis to re-run the entire pipeline for every combination of the specified parameters (in this case, 3 bin sizes * 2 window sizes = 6 total runs). The dot-notation in the keys of `param_grid` allows the function to navigate the nested `study_config` dictionary.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 3: Executing the Master Pipeline**\n",
        "\n",
        "With our data and configurations prepared, we can now execute the entire analysis with a single function call. We will enable all analyses and request the superior HTML report format.\n",
        "\n",
        "```python\n",
        "# --- Execute the Master Pipeline ---\n",
        "# This single function call runs all the analysis, interpretation, and reporting.\n",
        "# We set the optional flags to True to perform all available analyses.\n",
        "# We choose 'html' for the richest report format.\n",
        "\n",
        "# Note: This is a computationally intensive operation.\n",
        "master_results, html_report = run_master_pipeline(\n",
        "    raw_price_df=raw_price_df,\n",
        "    study_config=study_config,\n",
        "    param_grid=param_grid,\n",
        "    report_format='html',\n",
        "    run_robustness_analysis=True,\n",
        "    run_error_analysis=True,\n",
        "    bootstrap_samples=100 # A smaller number for a quick example; 500-1000 is better for rigor.\n",
        ")\n",
        "```\n",
        "**Discursive Text:** The `run_master_pipeline` function is called with all necessary inputs. It will now perform every task we have defined, from validation to bootstrapping. The function will return two objects: `master_results`, a deeply nested dictionary containing every single data artifact and result, and `html_report`, a string containing the full source code of a self-contained HTML report.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 4: Processing and Displaying the Outputs**\n",
        "\n",
        "The final step is to use the artifacts returned by the pipeline. We can save the HTML report to a file and programmatically access the results bundle to display specific findings or figures.\n",
        "\n",
        "```python\n",
        "# --- Process and Display Outputs ---\n",
        "\n",
        "# 1. Save the HTML report to a file\n",
        "# This file can be opened in any web browser to view the full interactive report.\n",
        "report_filename = \"financial_network_analysis_report.html\"\n",
        "with open(report_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_report)\n",
        "print(f\"\\nComprehensive HTML report saved to: {report_filename}\")\n",
        "\n",
        "# 2. Programmatically access and display a specific result\n",
        "# For example, let's display the table of robustly identified persistent TE links.\n",
        "print(\"\\n--- Most Robust Persistent Transfer Entropy Links ---\")\n",
        "robust_te_links = master_results['robustness_analysis']['persistent_links']['transfer_entropy']\n",
        "print(robust_te_links.head(10))\n",
        "\n",
        "# 3. Display one of the generated figures in an interactive environment (e.g., Jupyter)\n",
        "# This demonstrates how to access the matplotlib Figure objects.\n",
        "print(\"\\nDisplaying the static Kramers-Moyal network graph...\")\n",
        "km_network_fig = master_results['main_analysis']['visualizations']['static_km_graph']\n",
        "\n",
        "# In a Jupyter Notebook, simply having `km_network_fig` as the last line of a cell\n",
        "# would display the plot. Alternatively, you can explicitly show it or save it.\n",
        "# km_network_fig.show() # This would open the plot in a new window.\n",
        "km_network_fig.savefig(\"km_network_graph.png\", dpi=300)\n",
        "print(\"Saved 'km_network_graph.png'\")\n",
        "\n",
        "# Close all figures to free up memory\n",
        "plt.close('all')\n",
        "```\n",
        "**Discursive Text:** This final block demonstrates the utility of the pipeline's outputs. We save the rich HTML report for easy sharing and viewing. We then show how to programmatically dive into the `master_results` dictionary to extract a specific piece of data—the table of robust TE links—for further analysis or display. Finally, we access one of the `matplotlib.Figure` objects and save it to a high-quality PNG file, suitable for inclusion in a presentation or academic paper. This showcases the dual nature of the output: a human-readable report and a machine-readable data bundle for deeper, customized exploration."
      ],
      "metadata": {
        "id": "sJPkX_SxsEmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Parameter and Data Validation\n",
        "\n",
        "def _validate_dataframe(df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure and integrity of the input price DataFrame.\n",
        "\n",
        "    This helper function performs a series of checks to ensure the DataFrame\n",
        "    is suitable for the subsequent financial analysis pipeline. It verifies\n",
        "    the index type, data types of columns, and the presence of missing values.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame, expected to have a\n",
        "                           datetime index and numeric price columns.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates\n",
        "                   successful validation.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Step 1.1: Verify the index is a pandas DatetimeIndex.\n",
        "    # This is crucial for all time series operations.\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        # If the index is not a DatetimeIndex, append a specific error message.\n",
        "        errors.append(\"DataFrame index is not a DatetimeIndex. Please convert using pd.to_datetime().\")\n",
        "        # Return immediately as other checks depend on a valid DatetimeIndex.\n",
        "        return errors\n",
        "\n",
        "    # Step 1.2: Verify the index is sorted chronologically.\n",
        "    # A non-monotonic index can lead to incorrect shift/diff operations.\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        # If the index is not sorted, append a specific error message.\n",
        "        errors.append(\"DataFrame index is not monotonically increasing. Please sort the index.\")\n",
        "\n",
        "    # Step 1.3: Check that all columns contain numeric data.\n",
        "    # Non-numeric data would cause failures in mathematical operations like log-returns.\n",
        "    non_numeric_cols = df.select_dtypes(exclude=np.number).columns\n",
        "    # Check if there are any non-numeric columns.\n",
        "    if not non_numeric_cols.empty:\n",
        "        # If non-numeric columns are found, format a detailed error message.\n",
        "        errors.append(\n",
        "            f\"DataFrame contains non-numeric columns: {list(non_numeric_cols)}. \"\n",
        "            \"All columns must be of a numeric type (e.g., float64, int64).\"\n",
        "        )\n",
        "\n",
        "    # Step 1.4: Ensure there are no missing values (NaNs) in the DataFrame.\n",
        "    # Missing values must be handled explicitly in the preprocessing stage, not passed in.\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        # If any NaN values are present, identify which columns contain them.\n",
        "        nan_counts = df.isnull().sum()\n",
        "        cols_with_nan = nan_counts[nan_counts > 0].index.tolist()\n",
        "        # Append a detailed error message.\n",
        "        errors.append(f\"DataFrame contains missing values (NaNs) in columns: {cols_with_nan}.\")\n",
        "\n",
        "    # Return the list of all found errors.\n",
        "    return errors\n",
        "\n",
        "def _validate_study_config(config: Dict[str, Any]) -> Tuple[Dict[str, Any], List[str]]:\n",
        "    \"\"\"\n",
        "    Validates the structure, types, and values of the configuration dictionary.\n",
        "\n",
        "    This function recursively checks the configuration against a predefined\n",
        "    schema. It validates data types, value ranges, and converts date strings\n",
        "    to pd.Timestamp objects for downstream consistency.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, Any], List[str]]: A tuple containing the validated and\n",
        "                                         type-converted configuration dictionary\n",
        "                                         and a list of error messages.\n",
        "    \"\"\"\n",
        "    # Create a deep copy of the configuration to modify (e.g., convert dates).\n",
        "    validated_config = deepcopy(config)\n",
        "    # Initialize a list to collect error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Define the expected schema for validation. This is the \"golden\" structure.\n",
        "    schema = {\n",
        "        'data_params': {\n",
        "            'date_range': {'start': str, 'end': str},\n",
        "            'crisis_periods': dict\n",
        "        },\n",
        "        'analysis_params': {\n",
        "            'information_theory': {\n",
        "                'discretization_bins': int,\n",
        "                'discretization_strategy': str\n",
        "            },\n",
        "            'sliding_window': {\n",
        "                'window_size_days': int,\n",
        "                'step_size_days': int\n",
        "            },\n",
        "            'kramers_moyal': {\n",
        "                'model_assumption': str\n",
        "            }\n",
        "        },\n",
        "        'visualization_params': dict # Further checks are not schema-based\n",
        "    }\n",
        "\n",
        "    # Helper function to recursively check the schema.\n",
        "    def check_schema(conf_level, schema_level, path):\n",
        "        # Check if the current configuration level is a dictionary.\n",
        "        if not isinstance(conf_level, dict):\n",
        "            errors.append(f\"Configuration path '{' -> '.join(path)}' must be a dictionary.\")\n",
        "            return\n",
        "\n",
        "        # Iterate through the keys in the schema for the current level.\n",
        "        for key, expected_type in schema_level.items():\n",
        "            # Check if a required key is missing.\n",
        "            if key not in conf_level:\n",
        "                errors.append(f\"Missing required key '{key}' in path '{' -> '.join(path)}'.\")\n",
        "                continue\n",
        "\n",
        "            # If the expected type is a dictionary, recurse.\n",
        "            if expected_type == dict:\n",
        "                check_schema(conf_level[key], schema[key], path + [key])\n",
        "            # Otherwise, check if the value has the correct type.\n",
        "            elif not isinstance(conf_level[key], expected_type):\n",
        "                errors.append(\n",
        "                    f\"Invalid type for key '{key}' in path '{' -> '.join(path)}'. \"\n",
        "                    f\"Expected {expected_type.__name__}, got {type(conf_level[key]).__name__}.\"\n",
        "                )\n",
        "\n",
        "    # Start the recursive schema check from the top level.\n",
        "    check_schema(validated_config, schema, [])\n",
        "\n",
        "    # If schema validation fails, return early.\n",
        "    if errors:\n",
        "        return validated_config, errors\n",
        "\n",
        "    # --- Value-specific validations ---\n",
        "\n",
        "    # Validate 'data_params'\n",
        "    try:\n",
        "        # Convert date range strings to Timestamp objects.\n",
        "        start_date_str = validated_config['data_params']['date_range']['start']\n",
        "        end_date_str = validated_config['data_params']['date_range']['end']\n",
        "        start_date = pd.to_datetime(start_date_str)\n",
        "        end_date = pd.to_datetime(end_date_str)\n",
        "\n",
        "        # Check if the start date is before the end date.\n",
        "        if start_date >= end_date:\n",
        "            errors.append(\"In 'date_range', 'start' date must be before 'end' date.\")\n",
        "\n",
        "        # Update the config with Timestamp objects.\n",
        "        validated_config['data_params']['date_range']['start'] = start_date\n",
        "        validated_config['data_params']['date_range']['end'] = end_date\n",
        "\n",
        "        # Validate crisis periods.\n",
        "        for name, period in validated_config['data_params']['crisis_periods'].items():\n",
        "            # Convert crisis period dates to Timestamps.\n",
        "            crisis_start = pd.to_datetime(period['start'])\n",
        "            crisis_end = pd.to_datetime(period['end'])\n",
        "\n",
        "            # Check for valid temporal order.\n",
        "            if crisis_start >= crisis_end:\n",
        "                errors.append(f\"In crisis period '{name}', 'start' date must be before 'end' date.\")\n",
        "\n",
        "            # Check if the crisis period is within the main study date range.\n",
        "            if not (start_date <= crisis_start and crisis_end <= end_date):\n",
        "                errors.append(f\"Crisis period '{name}' is outside the main study 'date_range'.\")\n",
        "\n",
        "            # Update the config with Timestamp objects.\n",
        "            period['start'] = crisis_start\n",
        "            period['end'] = crisis_end\n",
        "\n",
        "    except (ValueError, TypeError) as e:\n",
        "        # Catch errors from pd.to_datetime if format is wrong.\n",
        "        errors.append(f\"Invalid date format in 'data_params'. Use 'YYYY-MM-DD'. Error: {e}\")\n",
        "\n",
        "    # Validate 'analysis_params'\n",
        "    # Check discretization_bins.\n",
        "    bins = validated_config['analysis_params']['information_theory']['discretization_bins']\n",
        "    if not (isinstance(bins, int) and bins > 0):\n",
        "        errors.append(\"'discretization_bins' must be a positive integer.\")\n",
        "\n",
        "    # Check sliding window parameters.\n",
        "    window_size = validated_config['analysis_params']['sliding_window']['window_size_days']\n",
        "    step_size = validated_config['analysis_params']['sliding_window']['step_size_days']\n",
        "    if not (isinstance(window_size, int) and window_size > 0):\n",
        "        errors.append(\"'window_size_days' must be a positive integer.\")\n",
        "    if not (isinstance(step_size, int) and step_size > 0):\n",
        "        errors.append(\"'step_size_days' must be a positive integer.\")\n",
        "    if window_size < step_size:\n",
        "        errors.append(\"'window_size_days' should be greater than or equal to 'step_size_days'.\")\n",
        "\n",
        "    # Return the validated config and any errors found.\n",
        "    return validated_config, errors\n",
        "\n",
        "def _cross_validate_inputs(df: pd.DataFrame, config: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs cross-validation between the DataFrame and the configuration.\n",
        "\n",
        "    This function ensures that the provided data is sufficient to perform the\n",
        "    analysis specified in the configuration, primarily by checking if the\n",
        "    DataFrame's date range covers the study's required date range.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The validated input DataFrame.\n",
        "        config (Dict[str, Any]): The validated and type-converted configuration\n",
        "                                 dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of error messages. An empty list indicates\n",
        "                   successful validation.\n",
        "    \"\"\"\n",
        "    # Initialize a list to collect error messages.\n",
        "    errors = []\n",
        "\n",
        "    # Extract the required date range from the configuration.\n",
        "    required_start = config['data_params']['date_range']['start']\n",
        "    required_end = config['data_params']['date_range']['end']\n",
        "\n",
        "    # Extract the available date range from the DataFrame's index.\n",
        "    available_start = df.index.min()\n",
        "    available_end = df.index.max()\n",
        "\n",
        "    # Check if the DataFrame's start date is later than the required start date.\n",
        "    if available_start > required_start:\n",
        "        errors.append(\n",
        "            f\"DataFrame data starts on {available_start.date()}, which is after the \"\n",
        "            f\"required study start date of {required_start.date()}.\"\n",
        "        )\n",
        "\n",
        "    # Check if the DataFrame's end date is earlier than the required end date.\n",
        "    if available_end < required_end:\n",
        "        errors.append(\n",
        "            f\"DataFrame data ends on {available_end.date()}, which is before the \"\n",
        "            f\"required study end date of {required_end.date()}.\"\n",
        "        )\n",
        "\n",
        "    # Return the list of all found errors.\n",
        "    return errors\n",
        "\n",
        "def validate_inputs(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of all inputs for the financial network analysis.\n",
        "\n",
        "    This master function validates the input DataFrame and the study\n",
        "    configuration dictionary by calling specialized helper functions. It ensures\n",
        "    that all inputs are structurally sound, internally consistent, and\n",
        "    compatible with each other. If validation is successful, it returns a\n",
        "    validated and type-converted configuration dictionary. Otherwise, it raises\n",
        "    a single comprehensive ValueError with all identified issues.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): A datetime-indexed DataFrame with daily closing\n",
        "                           prices of multiple assets in its columns.\n",
        "        config (Dict[str, Any]): A dictionary with study configuration\n",
        "                                 parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The validated configuration dictionary, with date\n",
        "                        strings converted to pd.Timestamp objects.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any validation check fails, this error is raised with\n",
        "                    a detailed list of all identified problems.\n",
        "        TypeError: If the inputs are not of the expected type.\n",
        "    \"\"\"\n",
        "    # --- Initial Type Checking ---\n",
        "    # Ensure df is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    # Ensure config is a dictionary.\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Validate DataFrame Structure ---\n",
        "    # Call the helper function to validate the DataFrame.\n",
        "    df_errors = _validate_dataframe(df)\n",
        "\n",
        "    # --- Step 2: Validate Configuration Dictionary ---\n",
        "    # Call the helper function to validate the configuration.\n",
        "    # This also returns a config with date strings converted to Timestamps.\n",
        "    validated_config, config_errors = _validate_study_config(config)\n",
        "\n",
        "    # --- Step 3: Cross-Validate DataFrame and Configuration ---\n",
        "    # Initialize cross-validation errors list.\n",
        "    cross_errors = []\n",
        "    # Only perform cross-validation if the individual inputs are valid.\n",
        "    if not df_errors and not config_errors:\n",
        "        cross_errors = _cross_validate_inputs(df, validated_config)\n",
        "\n",
        "    # --- Aggregate and Report Errors ---\n",
        "    # Combine all error messages from all validation steps.\n",
        "    all_errors = df_errors + config_errors + cross_errors\n",
        "\n",
        "    # Check if any errors were found.\n",
        "    if all_errors:\n",
        "        # If there are errors, compile them into a single, readable error message.\n",
        "        error_message = \"Input validation failed with the following errors:\\n\"\n",
        "        # Enumerate each error for clarity.\n",
        "        for i, error in enumerate(all_errors, 1):\n",
        "            error_message += f\"  {i}. {error}\\n\"\n",
        "        # Raise a single ValueError with the comprehensive report.\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "    # If no errors were found, print a success message.\n",
        "    print(\"Input validation successful. All checks passed.\")\n",
        "\n",
        "    # Return the validated and type-converted configuration dictionary.\n",
        "    return validated_config\n"
      ],
      "metadata": {
        "id": "3Ywks94YtmAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preprocessing\n",
        "\n",
        "def preprocess_price_data(\n",
        "    df: pd.DataFrame,\n",
        "    validated_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs a rigorous, sequential preprocessing of raw financial price data.\n",
        "\n",
        "    This function transforms a raw, datetime-indexed price DataFrame into a\n",
        "    clean, stationary DataFrame of log-returns, ready for advanced econometric\n",
        "    and information-theoretic analysis. The process is executed in a strict,\n",
        "    non-negotiable order:\n",
        "    1.  Temporal Filtering: Slices the DataFrame to the exact date range\n",
        "        specified in the configuration.\n",
        "    2.  Data Cleaning: Replaces infinities and systematically fills missing\n",
        "        values using forward-fill, then backward-fill.\n",
        "    3.  Log-Return Calculation: Transforms the non-stationary price series into\n",
        "        a more stationary log-return series, checking for data integrity\n",
        "        (non-positive prices) before transformation.\n",
        "    4.  Finalization: Removes any remaining NaN values to produce a perfectly\n",
        "        clean and aligned final dataset.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): A validated, datetime-indexed DataFrame with daily\n",
        "                           closing prices of multiple assets in its columns.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary\n",
        "                                           returned from the `validate_inputs`\n",
        "                                           function.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "        - A pandas DataFrame containing the clean, stationary log-returns.\n",
        "        - A metadata dictionary detailing the preprocessing steps, including\n",
        "          observation counts at each stage.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If non-positive prices are detected, which makes log-return\n",
        "                    calculation impossible.\n",
        "        ValueError: If any column consists entirely of NaN values after cleaning,\n",
        "                    indicating a severe data quality issue.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure df is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    # Ensure validated_config is a dictionary.\n",
        "    if not isinstance(validated_config, dict):\n",
        "        raise TypeError(\"Input 'validated_config' must be a dictionary.\")\n",
        "\n",
        "    # Initialize a dictionary to store metadata about the preprocessing steps.\n",
        "    metadata = {}\n",
        "    # Record the initial number of observations.\n",
        "    metadata['initial_observations'] = len(df)\n",
        "\n",
        "    # --- Step 1: Temporal Filtering ---\n",
        "    # Extract the required start and end dates from the validated configuration.\n",
        "    start_date = validated_config['data_params']['date_range']['start']\n",
        "    end_date = validated_config['data_params']['date_range']['end']\n",
        "\n",
        "    # Create a copy of the sliced DataFrame to avoid SettingWithCopyWarning.\n",
        "    # .loc is used for precise, label-based slicing on the DatetimeIndex.\n",
        "    processed_df = df.loc[start_date:end_date].copy()\n",
        "\n",
        "    # Record the number of observations after filtering to the study's date range.\n",
        "    metadata['observations_after_filtering'] = len(processed_df)\n",
        "    metadata['filtered_date_range'] = {'start': processed_df.index.min(), 'end': processed_df.index.max()}\n",
        "\n",
        "    # --- Step 2: Comprehensive Data Cleaning ---\n",
        "    # The cleaning sequence is critical and must be followed precisely.\n",
        "\n",
        "    # First, replace any infinite values with NaN. Infinite values are data errors.\n",
        "    processed_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Second, forward-fill NaN values. This carries the last valid observation\n",
        "    # forward, which is standard practice for handling market holidays and weekends.\n",
        "    processed_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # Third, backward-fill remaining NaN values. This handles any missing data\n",
        "    # at the very beginning of the series that forward-fill could not resolve.\n",
        "    # Issue: It may introduce a look-ahead bias to the data so it has been commented out.\n",
        "    # processed_df.fillna(method='bfill', inplace=True)\n",
        "\n",
        "    # After filling, check if any column is still entirely NaN.\n",
        "    if processed_df.isnull().all().any():\n",
        "        # This indicates a severe data issue (e.g., an asset with no data in the range).\n",
        "        bad_cols = processed_df.columns[processed_df.isnull().all()].tolist()\n",
        "        raise ValueError(\n",
        "            f\"Data cleaning failed. The following columns consist entirely of \"\n",
        "            f\"missing values within the specified date range: {bad_cols}\"\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Log-Return Calculation ---\n",
        "    # Before taking the logarithm, rigorously check for non-positive prices.\n",
        "    # The logarithm is undefined for zero or negative values.\n",
        "    if (processed_df <= 0).any().any():\n",
        "        # If found, raise a descriptive error to halt execution.\n",
        "        raise ValueError(\n",
        "            \"Preprocessing failed: DataFrame contains zero or negative prices, \"\n",
        "            \"which makes log-return calculation impossible. Please clean the source data.\"\n",
        "        )\n",
        "\n",
        "    # Calculate log-returns using the numerically stable formula: r_t = log(P_t / P_{t-1})\n",
        "    # This is equivalent to log(P_t) - log(P_{t-1})\n",
        "    log_returns_df = np.log(processed_df / processed_df.shift(1))\n",
        "\n",
        "    # --- Step 4: Final Data Structure Creation ---\n",
        "    # The first row of the log-returns DataFrame will be NaN due to the .shift(1) operation.\n",
        "    # .dropna() removes this row and any other rows with NaNs that might arise from\n",
        "    # misaligned trading calendars, ensuring a perfectly rectangular dataset.\n",
        "    final_df = log_returns_df.dropna()\n",
        "\n",
        "    # Record the final number of observations after all processing.\n",
        "    metadata['final_observations'] = len(final_df)\n",
        "    metadata['observations_dropped'] = metadata['initial_observations'] - len(final_df)\n",
        "\n",
        "    # Final sanity check: ensure the resulting DataFrame is completely free of non-finite values.\n",
        "    # This is a critical assertion for a production-grade system.\n",
        "    if not np.isfinite(final_df).all().all():\n",
        "        # This error should theoretically never be reached if the logic is correct,\n",
        "        # but serves as a final, critical guardrail.\n",
        "        raise RuntimeError(\n",
        "            \"A critical error occurred: the final preprocessed DataFrame \"\n",
        "            \"contains non-finite values (NaN or infinity).\"\n",
        "        )\n",
        "\n",
        "    # Print a success message with key metadata.\n",
        "    print(\n",
        "        f\"Data preprocessing successful. Final dataset contains {metadata['final_observations']} \"\n",
        "        f\"observations from {final_df.index.min().date()} to {final_df.index.max().date()}.\"\n",
        "    )\n",
        "\n",
        "    # Return the final, clean DataFrame of log-returns and the metadata dictionary.\n",
        "    return final_df, metadata\n"
      ],
      "metadata": {
        "id": "Y-MbdGK_t8Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Stationarity Check\n",
        "\n",
        "def _run_adf_test(\n",
        "    series: pd.Series,\n",
        "    significance_level: float = 0.05\n",
        ") -> Dict[str, Union[float, int, bool, str]]:\n",
        "    \"\"\"\n",
        "    Executes a single Augmented Dickey-Fuller (ADF) test on a time series.\n",
        "\n",
        "    This helper function provides a standardized, rigorous wrapper around the\n",
        "    statsmodels `adfuller` implementation. It uses a conservative regression\n",
        "    setting ('ct' - constant and trend) and data-driven lag selection ('AIC')\n",
        "    to ensure robust testing for financial time series. The results are\n",
        "    packaged into a dictionary with clear, human-readable interpretations.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The time series data to be tested for stationarity.\n",
        "        significance_level (float): The p-value threshold for rejecting the\n",
        "                                    null hypothesis. Defaults to 0.05.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Union[float, int, bool, str]]: A dictionary containing the\n",
        "        raw test outputs and a clear interpretation, including:\n",
        "        - 'adf_statistic': The computed test statistic.\n",
        "        - 'p_value': The p-value of the test.\n",
        "        - 'lags_used': The number of lags selected for the regression.\n",
        "        - 'is_stationary': A boolean indicating if the series is stationary\n",
        "                           at the given significance level.\n",
        "        - 'interpretation': A human-readable string explaining the result.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas Series.\n",
        "    if not isinstance(series, pd.Series):\n",
        "        raise TypeError(\"Input 'series' must be a pandas Series.\")\n",
        "    # Ensure the series is not empty.\n",
        "    if series.empty:\n",
        "        raise ValueError(\"Input 'series' cannot be empty.\")\n",
        "\n",
        "    # Execute the Augmented Dickey-Fuller test.\n",
        "    # regression='ct': Include constant and trend in the regression. This is a\n",
        "    #                  conservative choice suitable for financial price series\n",
        "    #                  which often exhibit drift and trend.\n",
        "    # autolag='AIC': Automatically select the optimal number of lags based on\n",
        "    #                the Akaike Information Criterion, making the test adaptive.\n",
        "    try:\n",
        "        adf_result = adfuller(series, regression='ct', autolag='AIC')\n",
        "    except Exception as e:\n",
        "        # Catch potential errors from the statsmodels library, e.g., with very short series.\n",
        "        raise RuntimeError(f\"ADF test failed for series '{series.name}' with error: {e}\")\n",
        "\n",
        "    # Extract the test statistic and p-value from the results tuple.\n",
        "    adf_statistic = adf_result[0]\n",
        "    p_value = adf_result[1]\n",
        "    lags_used = adf_result[2]\n",
        "\n",
        "    # Interpret the result based on the p-value.\n",
        "    # The null hypothesis (H0) of the ADF test is that the series has a unit root (is non-stationary).\n",
        "    # We reject H0 if the p-value is less than the significance level.\n",
        "    if p_value < significance_level:\n",
        "        # If p-value is low, we reject H0 and conclude the series is stationary.\n",
        "        is_stationary = True\n",
        "        interpretation = f\"Reject H0. Series is likely stationary (p-value: {p_value:.4f}).\"\n",
        "    else:\n",
        "        # If p-value is high, we fail to reject H0 and conclude the series is non-stationary.\n",
        "        is_stationary = False\n",
        "        interpretation = f\"Fail to reject H0. Series is likely non-stationary (p-value: {p_value:.4f}).\"\n",
        "\n",
        "    # Compile the results into a structured dictionary for clear output.\n",
        "    results_dict = {\n",
        "        'adf_statistic': adf_statistic,\n",
        "        'p_value': p_value,\n",
        "        'lags_used': lags_used,\n",
        "        'is_stationary': is_stationary,\n",
        "        'interpretation': interpretation\n",
        "    }\n",
        "\n",
        "    return results_dict\n",
        "\n",
        "def perform_stationarity_analysis(\n",
        "    price_df: pd.DataFrame,\n",
        "    log_return_df: pd.DataFrame,\n",
        "    significance_level: float = 0.05\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs and summarizes stationarity tests for price and log-return series.\n",
        "\n",
        "    This function systematically applies the Augmented Dickey-Fuller (ADF) test\n",
        "    to each asset's raw price series and its corresponding log-return series.\n",
        "    It serves to empirically validate the core econometric assumption that\n",
        "    prices are non-stationary while log-returns are stationary. The results\n",
        "    are compiled into a single, easily interpretable DataFrame.\n",
        "\n",
        "    Args:\n",
        "        price_df (pd.DataFrame): A clean, datetime-indexed DataFrame of asset\n",
        "                                 prices. Should align with the log_return_df.\n",
        "        log_return_df (pd.DataFrame): The DataFrame of log-returns generated\n",
        "                                      from the price_df.\n",
        "        significance_level (float): The statistical significance level for the\n",
        "                                    ADF test. Defaults to 0.05.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A multi-index DataFrame summarizing the ADF test results\n",
        "                      for both price and log-return series for each asset.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If inputs are not pandas DataFrames.\n",
        "        ValueError: If input DataFrames are empty or have misaligned columns.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(price_df, pd.DataFrame) or not isinstance(log_return_df, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs 'price_df' and 'log_return_df' must be pandas DataFrames.\")\n",
        "    if price_df.empty or log_return_df.empty:\n",
        "        raise ValueError(\"Input DataFrames cannot be empty.\")\n",
        "    if not all(price_df.columns == log_return_df.columns):\n",
        "        raise ValueError(\"Columns of 'price_df' and 'log_return_df' must be identical and in the same order.\")\n",
        "\n",
        "    # Initialize a list to store the results for each asset.\n",
        "    all_results = []\n",
        "    # Get the list of assets from the DataFrame columns.\n",
        "    assets = price_df.columns\n",
        "\n",
        "    # Iterate through each asset to perform the paired tests.\n",
        "    for asset in assets:\n",
        "        # Announce which asset is being tested.\n",
        "        print(f\"--- Running Stationarity Tests for: {asset} ---\")\n",
        "\n",
        "        # --- Test 1: Price Series ---\n",
        "        # Extract the price series for the current asset.\n",
        "        price_series = price_df[asset].dropna()\n",
        "        # Run the ADF test on the price series.\n",
        "        price_test_results = _run_adf_test(price_series, significance_level)\n",
        "        print(f\"  Price Series: {price_test_results['interpretation']}\")\n",
        "\n",
        "        # --- Test 2: Log-Return Series ---\n",
        "        # Extract the log-return series for the current asset.\n",
        "        return_series = log_return_df[asset].dropna()\n",
        "        # Run the ADF test on the log-return series.\n",
        "        return_test_results = _run_adf_test(return_series, significance_level)\n",
        "        print(f\"  Log-Return Series: {return_test_results['interpretation']}\")\n",
        "\n",
        "        # Combine the results for the current asset into a single dictionary.\n",
        "        # This flattened structure is ideal for creating a DataFrame later.\n",
        "        combined_results = {\n",
        "            ('Price', 'ADF Statistic'): price_test_results['adf_statistic'],\n",
        "            ('Price', 'P-Value'): price_test_results['p_value'],\n",
        "            ('Price', 'Is Stationary'): price_test_results['is_stationary'],\n",
        "            ('Log-Return', 'ADF Statistic'): return_test_results['adf_statistic'],\n",
        "            ('Log-Return', 'P-Value'): return_test_results['p_value'],\n",
        "            ('Log-Return', 'Is Stationary'): return_test_results['is_stationary'],\n",
        "        }\n",
        "        # Add the asset name for indexing.\n",
        "        combined_results['Asset'] = asset\n",
        "        # Append the asset's results to the master list.\n",
        "        all_results.append(combined_results)\n",
        "\n",
        "    # --- Compile Final Report ---\n",
        "    # Convert the list of dictionaries into a pandas DataFrame.\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    # Set the 'Asset' column as the DataFrame index.\n",
        "    results_df.set_index('Asset', inplace=True)\n",
        "    # Convert the flattened column names into a MultiIndex for clarity.\n",
        "    results_df.columns = pd.MultiIndex.from_tuples(results_df.columns)\n",
        "\n",
        "    # --- Final Conclusion Summary ---\n",
        "    # Programmatically verify the expected outcome for all assets.\n",
        "    prices_are_non_stationary = not results_df[('Price', 'Is Stationary')].any()\n",
        "    returns_are_stationary = results_df[('Log-Return', 'Is Stationary')].all()\n",
        "\n",
        "    print(\"\\n--- Stationarity Analysis Summary ---\")\n",
        "    if prices_are_non_stationary and returns_are_stationary:\n",
        "        # This is the desired outcome, confirming the validity of the log-return transformation.\n",
        "        print(\"SUCCESS: As expected, all price series were found to be non-stationary,\")\n",
        "        print(\"         and all log-return series were found to be stationary.\")\n",
        "        print(\"         The data is suitable for subsequent analysis.\")\n",
        "    else:\n",
        "        # This indicates a potential issue with the data or an unexpected market behavior.\n",
        "        print(\"WARNING: The stationarity analysis produced unexpected results.\")\n",
        "        if not prices_are_non_stationary:\n",
        "            print(\"  - One or more price series were unexpectedly found to be stationary.\")\n",
        "        if not returns_are_stationary:\n",
        "            print(\"  - One or more log-return series were unexpectedly found to be non-stationary.\")\n",
        "        print(\"         Proceed with caution and review the data and results carefully.\")\n",
        "\n",
        "    # Return the final, compiled DataFrame.\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "Mw1OvBszvfEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Descriptive Statistics\n",
        "\n",
        "def generate_descriptive_statistics(\n",
        "    log_return_df: pd.DataFrame,\n",
        "    validated_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, plt.Figure]:\n",
        "    \"\"\"\n",
        "    Calculates descriptive statistics and visualizes the log-return series.\n",
        "\n",
        "    This function performs two primary operations on the clean log-return data:\n",
        "    1.  It computes the first four statistical moments (Mean, Standard\n",
        "        Deviation, Skewness, and Excess Kurtosis) with high precision, using\n",
        "        unbiased estimators to ensure statistical rigor and comparability with\n",
        "        academic literature.\n",
        "    2.  It generates a publication-quality time series plot for each asset,\n",
        "        with key crisis periods annotated, providing a clear visual summary of\n",
        "        the data's behavior over time.\n",
        "\n",
        "    Args:\n",
        "        log_return_df (pd.DataFrame): A clean, stationary, datetime-indexed\n",
        "                                      DataFrame of log-returns.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary\n",
        "                                           containing parameters for analysis\n",
        "                                           and visualization.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, plt.Figure]:\n",
        "        - A pandas DataFrame containing the four calculated statistical\n",
        "          moments for each asset.\n",
        "        - A matplotlib Figure object containing the time series subplots for\n",
        "          each asset's log-returns.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If inputs are not of the expected types.\n",
        "        ValueError: If the log_return_df is empty.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(log_return_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'log_return_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(validated_config, dict):\n",
        "        raise TypeError(\"Input 'validated_config' must be a dictionary.\")\n",
        "    if log_return_df.empty:\n",
        "        raise ValueError(\"Input 'log_return_df' cannot be empty.\")\n",
        "\n",
        "    # --- Step 1: Four Moments Calculation ---\n",
        "    # Use a dictionary to build the statistics DataFrame column by column.\n",
        "    stats_data = {\n",
        "        # Calculate the mean return for each asset.\n",
        "        'Mean': log_return_df.mean(),\n",
        "        # Calculate the sample standard deviation (volatility). ddof=1 is crucial for sample std.\n",
        "        'Standard Deviation': log_return_df.std(ddof=1),\n",
        "        # Calculate the unbiased sample skewness using scipy.stats for precision.\n",
        "        # Formula: g1 = k3 / k2^(3/2)\n",
        "        'Skewness': scipy.stats.skew(log_return_df, bias=False),\n",
        "        # Calculate the unbiased sample excess kurtosis (Fisher's definition).\n",
        "        # Formula: g2 = k4 / k2^2\n",
        "        # 'fisher=True' subtracts 3 to measure kurtosis relative to a normal distribution.\n",
        "        'Excess Kurtosis': scipy.stats.kurtosis(log_return_df, bias=False, fisher=True)\n",
        "    }\n",
        "    # Create the final statistics DataFrame, transposing it for conventional format (moments as rows).\n",
        "    stats_df = pd.DataFrame(stats_data).T\n",
        "\n",
        "    # --- Step 2 & 3: Time Series Visualization ---\n",
        "    # Extract assets and visualization parameters from the configuration.\n",
        "    assets = log_return_df.columns\n",
        "    num_assets = len(assets)\n",
        "    viz_params = validated_config['visualization_params']\n",
        "\n",
        "    # Programmatically determine the subplot grid layout for optimal viewing.\n",
        "    # Aims for a layout that is as close to square as possible.\n",
        "    num_cols = int(np.ceil(np.sqrt(num_assets)))\n",
        "    num_rows = int(np.ceil(num_assets / num_cols))\n",
        "\n",
        "    # Create the figure and axes objects with specified DPI and size.\n",
        "    fig, axes = plt.subplots(\n",
        "        nrows=num_rows,\n",
        "        ncols=num_cols,\n",
        "        figsize=(5 * num_cols, 4 * num_rows),\n",
        "        dpi=viz_params['general']['figure_dpi'],\n",
        "        sharex=True # Share the x-axis for easier comparison.\n",
        "    )\n",
        "    # Flatten the axes array for easy iteration, regardless of grid shape.\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Iterate through each asset to create its subplot.\n",
        "    for i, asset in enumerate(assets):\n",
        "        ax = axes[i]\n",
        "        # Plot the log-return series for the current asset.\n",
        "        ax.plot(log_return_df.index, log_return_df[asset], linewidth=0.7, label=asset)\n",
        "\n",
        "        # Set the title for the subplot with the asset's name.\n",
        "        ax.set_title(asset, fontsize=viz_params['general']['font_size'] + 2)\n",
        "        # Set the y-axis label.\n",
        "        ax.set_ylabel(\"Log-Return\", fontsize=viz_params['general']['font_size'])\n",
        "        # Add a grid for better readability.\n",
        "        ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "        # Annotate crisis periods on the subplot.\n",
        "        for period_name, period_details in validated_config['data_params']['crisis_periods'].items():\n",
        "            # Use axvspan to create a shaded vertical region for the crisis period.\n",
        "            ax.axvspan(\n",
        "                period_details['start'],\n",
        "                period_details['end'],\n",
        "                color=viz_params['heatmaps']['crisis_line_color'],\n",
        "                alpha=0.15, # Use transparency to avoid obscuring the data.\n",
        "                label=f\"{period_details['label']}\" if i == 0 else \"\" # Label only on the first plot.\n",
        "            )\n",
        "\n",
        "    # Clean up empty subplots if the number of assets is not a perfect square.\n",
        "    for j in range(num_assets, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "\n",
        "    # Add a single legend for the entire figure to avoid redundancy.\n",
        "    # Handles are collected from the first subplot.\n",
        "    handles, labels = axes[0].get_legend_handles_labels()\n",
        "    # Use a dictionary to get unique labels for the legend.\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    fig.legend(by_label.values(), by_label.keys(), loc='upper right', fontsize=viz_params['general']['font_size'])\n",
        "\n",
        "    # Apply a tight layout to optimize spacing between subplots.\n",
        "    fig.tight_layout(rect=[0, 0, 0.9, 1]) # Adjust rect to make space for the figure legend.\n",
        "\n",
        "    # Set a main title for the entire figure.\n",
        "    fig.suptitle(\"Daily Log-Returns of Financial Assets\", fontsize=viz_params['general']['font_size'] + 4, y=0.99)\n",
        "\n",
        "    # Print a confirmation message.\n",
        "    print(\"Descriptive statistics calculated and visualization generated successfully.\")\n",
        "\n",
        "    # Return the statistics DataFrame and the matplotlib Figure object.\n",
        "    return stats_df, fig\n"
      ],
      "metadata": {
        "id": "iZ5320aewZEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Transfer Entropy (TE) Analysis\n",
        "\n",
        "def _discretize_data(\n",
        "    log_return_df: pd.DataFrame,\n",
        "    num_bins: int\n",
        ") -> Tuple[pd.DataFrame, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Discretizes continuous log-return data into a specified number of bins.\n",
        "\n",
        "    This helper function uses equal-frequency binning (quantiles) to transform\n",
        "    continuous time series data into discrete integer labels. This is a\n",
        "    necessary prerequisite for estimating probability distributions for\n",
        "    Transfer Entropy calculation.\n",
        "\n",
        "    Args:\n",
        "        log_return_df (pd.DataFrame): A clean, stationary DataFrame of log-returns.\n",
        "        num_bins (int): The number of discrete bins to create.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, np.ndarray]]:\n",
        "        - A pandas DataFrame of the same shape as the input, containing\n",
        "          integer bin labels (from 0 to num_bins-1).\n",
        "        - A dictionary mapping each asset name to the computed bin edges.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of unique values in any series is less\n",
        "                    than the number of bins, making quantile-based\n",
        "                    discretization impossible.\n",
        "    \"\"\"\n",
        "    # Initialize a DataFrame to store the binned data.\n",
        "    binned_df = pd.DataFrame(index=log_return_df.index)\n",
        "    # Initialize a dictionary to store the bin edges for each asset.\n",
        "    bin_edges = {}\n",
        "\n",
        "    # Iterate over each asset column in the log-return DataFrame.\n",
        "    for col in log_return_df.columns:\n",
        "        # Pre-validation: Ensure meaningful discretization is possible.\n",
        "        if log_return_df[col].nunique() < num_bins:\n",
        "            raise ValueError(\n",
        "                f\"Cannot create {num_bins} unique bins for asset '{col}' as it \"\n",
        "                f\"only has {log_return_df[col].nunique()} unique values. \"\n",
        "                \"Consider reducing the number of bins.\"\n",
        "            )\n",
        "\n",
        "        # Use pandas.qcut for equal-frequency binning.\n",
        "        # `labels=False` returns integer bin identifiers.\n",
        "        # `duplicates='drop'` handles non-unique bin edges by merging them.\n",
        "        binned_data, edges = pd.qcut(\n",
        "            log_return_df[col],\n",
        "            q=num_bins,\n",
        "            labels=False,\n",
        "            duplicates='drop',\n",
        "            retbins=True # Return the computed bin edges.\n",
        "        )\n",
        "        # Store the resulting binned series in the new DataFrame.\n",
        "        binned_df[col] = binned_data\n",
        "        # Store the bin edges for reproducibility and interpretation.\n",
        "        bin_edges[col] = edges\n",
        "\n",
        "    # The result of qcut can be float if there are NaNs, so cast to a nullable integer type.\n",
        "    return binned_df.astype('Int64'), bin_edges\n",
        "\n",
        "def calculate_transfer_entropy(\n",
        "    log_return_df: pd.DataFrame,\n",
        "    validated_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the Transfer Entropy (TE) matrix with a direct formula implementation.\n",
        "\n",
        "    This function implements the complete TE pipeline, quantifying the directed\n",
        "    flow of information between all pairs of assets. This revised version\n",
        "    adheres strictly to the defining formula for TE (Eq. 2 in the source paper)\n",
        "    for maximum clarity and methodological fidelity.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  Discretizes the continuous log-return data into integer-labeled bins.\n",
        "    2.  Constructs the 3D joint probability distribution P(i_t+1, i_t, j_t).\n",
        "    3.  Explicitly calculates all required marginal probabilities.\n",
        "    4.  Calculates the conditional probabilities P(i_t+1|i_t, j_t) and P(i_t+1|i_t)\n",
        "        using numerically stable division.\n",
        "    5.  Computes the log-ratio of these conditional probabilities.\n",
        "    6.  Calculates the final TE value as the expectation of this log-ratio.\n",
        "\n",
        "    Args:\n",
        "        log_return_df (pd.DataFrame): A clean, stationary, datetime-indexed\n",
        "                                      DataFrame of log-returns.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An N x N DataFrame representing the Transfer Entropy\n",
        "                      matrix, where N is the number of assets. The element\n",
        "                      at (row_i, col_j) is the TE from asset j to asset i.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(log_return_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'log_return_df' must be a pandas DataFrame.\")\n",
        "    if log_return_df.empty:\n",
        "        raise ValueError(\"Input 'log_return_df' cannot be empty.\")\n",
        "\n",
        "    # --- Step 1: Discretization ---\n",
        "    # Extract the number of bins from the configuration.\n",
        "    num_bins = validated_config['analysis_params']['information_theory']['discretization_bins']\n",
        "    # Call the helper function to discretize the data.\n",
        "    binned_df, _ = _discretize_data(log_return_df, num_bins)\n",
        "\n",
        "    # Get the list of assets and the number of assets.\n",
        "    assets = log_return_df.columns\n",
        "    num_assets = len(assets)\n",
        "    # Define a small constant for numerical stability.\n",
        "    epsilon = 1e-15\n",
        "\n",
        "    # Initialize the N x N Transfer Entropy matrix with zeros.\n",
        "    te_matrix = pd.DataFrame(np.zeros((num_assets, num_assets)), index=assets, columns=assets)\n",
        "\n",
        "    # Iterate over every ordered pair of assets (i, j) to calculate TE from j to i.\n",
        "    for i_idx, i_asset in enumerate(assets):  # i is the target series\n",
        "        for j_idx, j_asset in enumerate(assets):  # j is the source series\n",
        "            # TE from a series to itself is not calculated.\n",
        "            if i_asset == j_asset:\n",
        "                continue\n",
        "\n",
        "            print(f\"Calculating Transfer Entropy from {j_asset} -> {i_asset}...\")\n",
        "\n",
        "            # Prepare the data vectors for i_{t+1}, i_t, and j_t.\n",
        "            i_future = binned_df[i_asset].values[1:]\n",
        "            i_present = binned_df[i_asset].values[:-1]\n",
        "            j_present = binned_df[j_asset].values[:-1]\n",
        "            sample_data = np.stack([i_future, i_present, j_present], axis=1)\n",
        "\n",
        "            # --- Step 2: Estimate 3D Joint Probability Distribution ---\n",
        "            # P(i_{t+1}, i_t, j_t)\n",
        "            joint_counts, _ = np.histogramdd(\n",
        "                sample_data, bins=num_bins, range=[(-0.5, num_bins - 0.5)] * 3\n",
        "            )\n",
        "            p_joint_3d = joint_counts / np.sum(joint_counts)\n",
        "\n",
        "            # --- Step 3: Explicitly Calculate All Required Marginal Probabilities ---\n",
        "            # P(i_t, j_t) - sum over the i_{t+1} axis (axis 0)\n",
        "            p_ip_jp = p_joint_3d.sum(axis=0)\n",
        "            # P(i_{t+1}, i_t) - sum over the j_t axis (axis 2)\n",
        "            p_if_ip = p_joint_3d.sum(axis=2)\n",
        "            # P(i_t) - sum over the j_t axis (axis 1) of P(i_t, j_t)\n",
        "            p_ip = p_ip_jp.sum(axis=1)\n",
        "\n",
        "            # --- Step 4: Calculate Conditional Probabilities with Numerical Stability ---\n",
        "            # P(i_{t+1} | i_t, j_t) = P(i_{t+1}, i_t, j_t) / P(i_t, j_t)\n",
        "            # Reshape denominator for broadcasting: (B, B) -> (1, B, B)\n",
        "            p_ip_jp_reshaped = p_ip_jp[np.newaxis, :, :]\n",
        "            p_cond_if_given_ip_jp = np.divide(\n",
        "                p_joint_3d, p_ip_jp_reshaped,\n",
        "                out=np.zeros_like(p_joint_3d), where=(p_ip_jp_reshaped > epsilon)\n",
        "            )\n",
        "\n",
        "            # P(i_{t+1} | i_t) = P(i_{t+1}, i_t) / P(i_t)\n",
        "            # Reshape denominator for broadcasting: (B,) -> (1, B)\n",
        "            p_ip_reshaped = p_ip[:, np.newaxis]\n",
        "            p_cond_if_given_ip = np.divide(\n",
        "                p_if_ip, p_ip_reshaped,\n",
        "                out=np.zeros_like(p_if_ip), where=(p_ip_reshaped > epsilon)\n",
        "            )\n",
        "\n",
        "            # --- Step 5: Calculate the Log-Ratio of Conditional Probabilities ---\n",
        "            # log2[ P(i_{t+1}|i_t,j_t) / P(i_{t+1}|i_t) ]\n",
        "            # Reshape P(i_{t+1}|i_t) for broadcasting: (B, B) -> (B, B, 1)\n",
        "            p_cond_if_given_ip_reshaped = p_cond_if_given_ip[:, :, np.newaxis]\n",
        "\n",
        "            # Calculate the ratio of conditional probabilities.\n",
        "            ratio_of_conds = np.divide(\n",
        "                p_cond_if_given_ip_jp, p_cond_if_given_ip_reshaped,\n",
        "                out=np.zeros_like(p_cond_if_given_ip_jp), where=(p_cond_if_given_ip_reshaped > epsilon)\n",
        "            )\n",
        "\n",
        "            # Calculate the log2 of the ratio.\n",
        "            log_ratio = np.log2(\n",
        "                ratio_of_conds,\n",
        "                out=np.zeros_like(ratio_of_conds), where=(ratio_of_conds > epsilon)\n",
        "            )\n",
        "\n",
        "            # --- Step 6: Compute the Final Summation (Expectation) ---\n",
        "            # TE = sum( P(i_{t+1}, i_t, j_t) * log_ratio )\n",
        "            te_value = np.sum(p_joint_3d * log_ratio)\n",
        "\n",
        "            # Store the calculated TE value in the matrix.\n",
        "            te_matrix.iloc[i_idx, j_idx] = te_value\n",
        "\n",
        "    print(\"\\nTransfer Entropy matrix calculation complete.\")\n",
        "    return te_matrix"
      ],
      "metadata": {
        "id": "LygtXhQkxEJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Kramers-Moyal (KM) Expansion\n",
        "\n",
        "def calculate_kramers_moyal_drift_matrix(\n",
        "    log_return_df: pd.DataFrame,\n",
        "    validated_config: Dict[str, Any] # Retained for future compatibility, not used here.\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estimates the linear drift matrix using the Kramers-Moyal expansion.\n",
        "\n",
        "    This function approximates the deterministic component of the financial\n",
        "    system's dynamics by calculating the first Kramers-Moyal coefficient,\n",
        "    the drift matrix A. It assumes a linear relationship where the expected\n",
        "    change in an asset's return is a linear combination of the current returns\n",
        "    of all assets in the system.\n",
        "\n",
        "    The estimation follows the methodology derived from the paper's moment\n",
        "    conditions (Eq. 7-9), which results in solving the linear matrix equation:\n",
        "    A = C_yx * C_xx^(-1)\n",
        "    where:\n",
        "    - A is the N x N drift coefficient matrix.\n",
        "    - C_yx is the time-lagged cross-covariance matrix between the increments\n",
        "      of the returns and the returns themselves: <(x(t+1)-x(t)) * x(t)^T>.\n",
        "    - C_xx is the standard covariance matrix of the returns: <x(t) * x(t)^T>.\n",
        "\n",
        "    Args:\n",
        "        log_return_df (pd.DataFrame): A clean, stationary, datetime-indexed\n",
        "                                      DataFrame of log-returns.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary.\n",
        "                                           (Currently unused but included for\n",
        "                                           API consistency).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An N x N DataFrame representing the estimated drift\n",
        "                      matrix A. The element at (row_i, col_j) represents the\n",
        "                      linear influence of asset j's return on the expected\n",
        "                      change in asset i's return.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If the input DataFrame is empty or has too few rows.\n",
        "        LinAlgError: If the covariance matrix is singular or ill-conditioned,\n",
        "                     making a reliable estimation of the drift matrix impossible.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(log_return_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'log_return_df' must be a pandas DataFrame.\")\n",
        "    if log_return_df.shape[0] < log_return_df.shape[1] * 2:\n",
        "        raise ValueError(\n",
        "            \"Input DataFrame has too few observations to reliably estimate \"\n",
        "            \"the covariance matrix. Need at least 2 * num_assets rows.\"\n",
        "        )\n",
        "\n",
        "    print(\"Calculating Kramers-Moyal drift matrix...\")\n",
        "\n",
        "    # --- Step 1: Increment Variable Calculation and Data Alignment ---\n",
        "    # Let x(t) be the log-return series.\n",
        "    x_df = log_return_df\n",
        "\n",
        "    # Calculate the one-step increment series y(t) = x(t+1) - x(t).\n",
        "    # .diff() calculates x(t) - x(t-1).\n",
        "    # .shift(-1) aligns this to represent x(t+1) - x(t).\n",
        "    y_df = x_df.diff().shift(-1)\n",
        "\n",
        "    # The last row of y_df will be NaN. We must drop the last row from both\n",
        "    # DataFrames to ensure they are perfectly aligned for moment calculations.\n",
        "    x_aligned = x_df.iloc[:-1]\n",
        "    y_aligned = y_df.iloc[:-1]\n",
        "\n",
        "    # --- Step 2: Moment Matrix Construction ---\n",
        "    # Convert to numpy arrays for efficient linear algebra.\n",
        "    x = x_aligned.values\n",
        "    y = y_aligned.values\n",
        "\n",
        "    # Get the number of observations (T) and number of assets (N).\n",
        "    T, N = x.shape\n",
        "\n",
        "    # Calculate the covariance matrix of the log-returns, C_xx = <x * x^T>.\n",
        "    # We use the direct formula (1/T) * x.T @ x for consistency with the paper's derivation.\n",
        "    # This is equivalent to pandas .cov() with ddof=0, but more explicit.\n",
        "    c_xx = (x.T @ x) / T\n",
        "\n",
        "    # Calculate the time-lagged cross-covariance matrix, C_yx = <y * x^T>.\n",
        "    c_yx = (y.T @ x) / T\n",
        "\n",
        "    # --- Step 3 & 4: Linear System Solution and Drift Matrix Assembly ---\n",
        "    # Before solving, check if the covariance matrix C_xx is well-conditioned.\n",
        "    # A high condition number indicates multicollinearity, making the inverse unstable.\n",
        "    condition_number = np.linalg.cond(c_xx)\n",
        "    # Define a threshold for an ill-conditioned matrix.\n",
        "    # sys.float_info.epsilon is the machine epsilon for float precision.\n",
        "    ill_condition_threshold = 1 / np.finfo(float).eps\n",
        "\n",
        "    if condition_number > ill_condition_threshold:\n",
        "        # If the matrix is ill-conditioned, raise a linear algebra error.\n",
        "        raise LinAlgError(\n",
        "            f\"Covariance matrix is singular or ill-conditioned \"\n",
        "            f\"(condition number: {condition_number:.2e}). The drift matrix \"\n",
        "            \"cannot be reliably estimated due to high asset correlation.\"\n",
        "        )\n",
        "\n",
        "    # Solve the matrix equation A = C_yx * C_xx^(-1) for the drift matrix A.\n",
        "    # Using np.linalg.solve(C_xx.T, C_yx.T).T is a robust way to compute this.\n",
        "    # It solves A * C_xx = C_yx, which is equivalent to A = C_yx * C_xx_inv.\n",
        "    drift_matrix_values = np.linalg.solve(c_xx.T, c_yx.T).T\n",
        "\n",
        "    # Convert the resulting numpy array back to a labeled pandas DataFrame.\n",
        "    drift_matrix_df = pd.DataFrame(\n",
        "        drift_matrix_values,\n",
        "        index=log_return_df.columns,\n",
        "        columns=log_return_df.columns\n",
        "    )\n",
        "\n",
        "    print(\"Kramers-Moyal drift matrix calculation complete.\")\n",
        "\n",
        "    # Return the final, labeled drift matrix.\n",
        "    return drift_matrix_df\n"
      ],
      "metadata": {
        "id": "REm_srW3xvqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Time-Resolved Analysis\n",
        "\n",
        "def perform_time_resolved_analysis(\n",
        "    log_return_df: pd.DataFrame,\n",
        "    validated_config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[pd.Timestamp, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Performs time-resolved TE and KM analysis using a sliding window approach.\n",
        "\n",
        "    This function brings a dynamic dimension to the analysis by repeatedly\n",
        "    applying the static Transfer Entropy and Kramers-Moyal calculations on\n",
        "    overlapping windows of the data. This process generates a time series of\n",
        "    network matrices, revealing the evolution of market structure and\n",
        "    interdependencies over time.\n",
        "\n",
        "    The process is as follows:\n",
        "    1.  A sliding window is defined by size and step parameters from the config.\n",
        "    2.  The window moves sequentially through the log-return time series.\n",
        "    3.  For each window, it robustly attempts to calculate both the TE and KM\n",
        "        drift matrices using the previously defined functions.\n",
        "    4.  Failures within a single window (e.g., due to ill-conditioned data)\n",
        "        are caught and logged, allowing the analysis to continue.\n",
        "    5.  Results are stored in a dictionary, indexed by the end-date of each\n",
        "        window, creating a time-stamped history of network snapshots.\n",
        "\n",
        "    Args:\n",
        "        log_return_df (pd.DataFrame): A clean, stationary, datetime-indexed\n",
        "                                      DataFrame of log-returns.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary\n",
        "                                           containing sliding window parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[pd.Timestamp, pd.DataFrame]]: A dictionary with two keys,\n",
        "        'transfer_entropy' and 'kramers_moyal'. Each key maps to another\n",
        "        dictionary where keys are timestamps (window end dates) and values\n",
        "        are the corresponding N x N network matrices (as pd.DataFrames).\n",
        "        A value may be None if the calculation for that window failed.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(log_return_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'log_return_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(validated_config, dict):\n",
        "        raise TypeError(\"Input 'validated_config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Sliding Window Framework Setup ---\n",
        "    # Extract sliding window parameters from the configuration.\n",
        "    window_params = validated_config['analysis_params']['sliding_window']\n",
        "    window_size = window_params['window_size_days']\n",
        "    step_size = window_params['step_size_days']\n",
        "\n",
        "    # Get the total number of observations available.\n",
        "    total_observations = len(log_return_df)\n",
        "\n",
        "    # Validate that the window size is not larger than the total dataset.\n",
        "    if window_size > total_observations:\n",
        "        raise ValueError(\n",
        "            f\"Window size ({window_size}) is larger than the total number of \"\n",
        "            f\"observations ({total_observations}).\"\n",
        "        )\n",
        "\n",
        "    # Initialize dictionaries to store the time-resolved results.\n",
        "    # The keys will be the timestamp of the window's end date.\n",
        "    time_resolved_results = {\n",
        "        'transfer_entropy': {},\n",
        "        'kramers_moyal': {}\n",
        "    }\n",
        "\n",
        "    # --- Step 2 & 3: Iterative Computation and Temporal Storage ---\n",
        "    # Define the start and end points for the sliding window loop.\n",
        "    start_indices = range(0, total_observations - window_size + 1, step_size)\n",
        "\n",
        "    print(f\"Starting time-resolved analysis with window size {window_size} and step size {step_size}...\")\n",
        "    # Use tqdm for a user-friendly progress bar during the long computation.\n",
        "    for i in tqdm(start_indices, desc=\"Processing Sliding Windows\"):\n",
        "        # Define the slice for the current window using integer location.\n",
        "        window_start_idx = i\n",
        "        window_end_idx = i + window_size\n",
        "        window_df = log_return_df.iloc[window_start_idx:window_end_idx]\n",
        "\n",
        "        # The timestamp for this window's results is its end date.\n",
        "        window_timestamp = window_df.index[-1]\n",
        "\n",
        "        # --- Calculate Transfer Entropy for the current window ---\n",
        "        try:\n",
        "            # Call the previously defined function on the window's data.\n",
        "            te_matrix = calculate_transfer_entropy(window_df, validated_config)\n",
        "            # Store the resulting matrix with its corresponding timestamp.\n",
        "            time_resolved_results['transfer_entropy'][window_timestamp] = te_matrix\n",
        "        except Exception as e:\n",
        "            # If any error occurs (e.g., not enough unique values for binning),\n",
        "            # log the error and store None for this window.\n",
        "            print(f\"\\nWarning: TE calculation failed for window ending {window_timestamp.date()}: {e}\")\n",
        "            time_resolved_results['transfer_entropy'][window_timestamp] = None\n",
        "\n",
        "        # --- Calculate Kramers-Moyal Drift Matrix for the current window ---\n",
        "        try:\n",
        "            # Call the previously defined function on the window's data.\n",
        "            km_matrix = calculate_kramers_moyal_drift_matrix(window_df, validated_config)\n",
        "            # Store the resulting matrix with its corresponding timestamp.\n",
        "            time_resolved_results['kramers_moyal'][window_timestamp] = km_matrix\n",
        "        except LinAlgError as e:\n",
        "            # Specifically catch linear algebra errors (e.g., singular matrix).\n",
        "            # This is a common issue in windows with low volatility or high correlation.\n",
        "            print(f\"\\nWarning: KM calculation failed for window ending {window_timestamp.date()}: {e}\")\n",
        "            time_resolved_results['kramers_moyal'][window_timestamp] = None\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected errors.\n",
        "            print(f\"\\nWarning: An unexpected error occurred in KM calculation for window ending {window_timestamp.date()}: {e}\")\n",
        "            time_resolved_results['kramers_moyal'][window_timestamp] = None\n",
        "\n",
        "    # --- Step 4: Temporal Dynamics Extraction (Implicit) ---\n",
        "    # The returned dictionary of matrices is the primary 3D data structure.\n",
        "    # Further transformation into a 2D DataFrame of time series can be done\n",
        "    # as a separate post-processing step, providing flexibility.\n",
        "    # This function's core responsibility is generating the time-stamped matrices.\n",
        "\n",
        "    print(\"\\nTime-resolved analysis complete.\")\n",
        "\n",
        "    # Return the final dictionary containing the time series of network matrices.\n",
        "    return time_resolved_results\n"
      ],
      "metadata": {
        "id": "XN4jp8wgylOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Crisis Period Analysis\n",
        "\n",
        "def analyze_crisis_periods(\n",
        "    time_resolved_results: Dict[str, Dict[pd.Timestamp, pd.DataFrame]],\n",
        "    validated_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes and quantifies the impact of specified crisis periods on network structures.\n",
        "\n",
        "    This function ingests the time-resolved network matrices and systematically\n",
        "    compares the network's average behavior during defined crisis periods against\n",
        "    a \"normal\" baseline period. It performs three key steps:\n",
        "    1.  Partitions the time-stamped matrices into distinct sets for each crisis\n",
        "        period and a baseline (non-crisis) period.\n",
        "    2.  Aggregates the matrices within each period by calculating the element-wise\n",
        "        mean, yielding an average network snapshot for each state.\n",
        "    3.  Calculates the percentage change of the crisis-period averages relative\n",
        "        to the baseline average, quantifying the magnitude of the crisis impact.\n",
        "\n",
        "    Args:\n",
        "        time_resolved_results (Dict[str, Dict[pd.Timestamp, pd.DataFrame]]):\n",
        "            The output from `perform_time_resolved_analysis`, containing the\n",
        "            time series of TE and KM matrices.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary,\n",
        "                                           containing crisis period definitions.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the analysis results.\n",
        "        Structure:\n",
        "        {\n",
        "            'transfer_entropy': {\n",
        "                'baseline': {'avg_matrix': pd.DataFrame, 'num_windows': int},\n",
        "                'crisis_period_1': {'avg_matrix': pd.DataFrame, 'pct_change': pd.DataFrame, 'num_windows': int},\n",
        "                ...\n",
        "            },\n",
        "            'kramers_moyal': { ... }\n",
        "        }\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If no valid windows are found for the baseline or any\n",
        "                    defined crisis period.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(time_resolved_results, dict) or not all(k in time_resolved_results for k in ['transfer_entropy', 'kramers_moyal']):\n",
        "        raise TypeError(\"time_resolved_results must be a dictionary with keys 'transfer_entropy' and 'kramers_moyal'.\")\n",
        "    if not isinstance(validated_config, dict):\n",
        "        raise TypeError(\"validated_config must be a dictionary.\")\n",
        "\n",
        "    print(\"Starting crisis period analysis...\")\n",
        "    # The main dictionary to store all analysis results.\n",
        "    analysis_output = {}\n",
        "    # Define a small epsilon for safe division when calculating percentage change.\n",
        "    epsilon = 1e-9\n",
        "\n",
        "    # Get the crisis period definitions from the configuration.\n",
        "    crisis_periods_config = validated_config['data_params']['crisis_periods']\n",
        "\n",
        "    # Iterate over each type of analysis (TE and KM).\n",
        "    for analysis_type, results_dict in time_resolved_results.items():\n",
        "        print(f\"--- Analyzing: {analysis_type.replace('_', ' ').title()} ---\")\n",
        "\n",
        "        # Initialize storage for this analysis type.\n",
        "        analysis_output[analysis_type] = {}\n",
        "\n",
        "        # --- Step 1: Partition Windows into Crisis and Baseline Sets ---\n",
        "        # First, identify all timestamps that belong to any crisis period.\n",
        "        all_crisis_timestamps = set()\n",
        "        for period_details in crisis_periods_config.values():\n",
        "            for ts in results_dict.keys():\n",
        "                if period_details['start'] <= ts <= period_details['end']:\n",
        "                    all_crisis_timestamps.add(ts)\n",
        "\n",
        "        # The baseline consists of all timestamps NOT in any crisis period.\n",
        "        baseline_timestamps = [ts for ts in results_dict.keys() if ts not in all_crisis_timestamps]\n",
        "\n",
        "        # Collect the actual matrix data, filtering out any None values from failed window calculations.\n",
        "        baseline_matrices = [results_dict[ts] for ts in baseline_timestamps if results_dict[ts] is not None]\n",
        "\n",
        "        # --- Step 2: Aggregate Baseline Period ---\n",
        "        if not baseline_matrices:\n",
        "            raise ValueError(f\"No valid data windows found for the baseline period in '{analysis_type}' analysis.\")\n",
        "\n",
        "        # Calculate the average baseline matrix by taking the mean along the temporal axis (axis=0).\n",
        "        avg_baseline_matrix = np.mean(np.stack(baseline_matrices, axis=0), axis=0)\n",
        "        # Convert the numpy array back to a labeled DataFrame.\n",
        "        avg_baseline_df = pd.DataFrame(avg_baseline_matrix, index=baseline_matrices[0].index, columns=baseline_matrices[0].columns)\n",
        "\n",
        "        # Store the baseline results.\n",
        "        analysis_output[analysis_type]['baseline'] = {\n",
        "            'avg_matrix': avg_baseline_df,\n",
        "            'num_windows': len(baseline_matrices)\n",
        "        }\n",
        "        print(f\"  Baseline: Calculated average from {len(baseline_matrices)} windows.\")\n",
        "\n",
        "        # --- Step 2 & 3: Aggregate Crisis Periods and Calculate Percentage Change ---\n",
        "        # Now, process each defined crisis period individually.\n",
        "        for period_name, period_details in crisis_periods_config.items():\n",
        "            # Identify timestamps for this specific crisis period.\n",
        "            crisis_timestamps = [\n",
        "                ts for ts in results_dict.keys()\n",
        "                if period_details['start'] <= ts <= period_details['end']\n",
        "            ]\n",
        "            # Collect the matrices, again filtering out Nones.\n",
        "            crisis_matrices = [results_dict[ts] for ts in crisis_timestamps if results_dict[ts] is not None]\n",
        "\n",
        "            if not crisis_matrices:\n",
        "                print(f\"  Warning: No valid data windows found for crisis period '{period_name}'. Skipping analysis for this period.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate the average crisis matrix.\n",
        "            avg_crisis_matrix = np.mean(np.stack(crisis_matrices, axis=0), axis=0)\n",
        "            avg_crisis_df = pd.DataFrame(avg_crisis_matrix, index=crisis_matrices[0].index, columns=crisis_matrices[0].columns)\n",
        "\n",
        "            # Calculate percentage change: 100 * (crisis - baseline) / baseline\n",
        "            # Use robust division to handle cases where the baseline value is near zero.\n",
        "            numerator = 100 * (avg_crisis_matrix - avg_baseline_matrix)\n",
        "            denominator = avg_baseline_matrix\n",
        "\n",
        "            # Create a mask to perform division only where the denominator is not close to zero.\n",
        "            where_safe = np.abs(denominator) > epsilon\n",
        "            # Initialize output array for percentage change.\n",
        "            pct_change_matrix = np.zeros_like(numerator)\n",
        "            # Perform the division only on the safe elements.\n",
        "            np.divide(numerator, denominator, out=pct_change_matrix, where=where_safe)\n",
        "\n",
        "            # Convert to a labeled DataFrame.\n",
        "            pct_change_df = pd.DataFrame(pct_change_matrix, index=avg_baseline_df.index, columns=avg_baseline_df.columns)\n",
        "\n",
        "            # Store all results for this crisis period.\n",
        "            analysis_output[analysis_type][period_name] = {\n",
        "                'avg_matrix': avg_crisis_df,\n",
        "                'pct_change_vs_baseline': pct_change_df,\n",
        "                'num_windows': len(crisis_matrices)\n",
        "            }\n",
        "            print(f\"  Crisis '{period_details['label']}': Calculated average from {len(crisis_matrices)} windows.\")\n",
        "\n",
        "    print(\"\\nCrisis period analysis complete.\")\n",
        "    return analysis_output\n"
      ],
      "metadata": {
        "id": "72E9FA8O0k5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Network Visualization\n",
        "\n",
        "def plot_matrix_heatmap(\n",
        "    matrix_df: pd.DataFrame,\n",
        "    title: str,\n",
        "    validated_config: Dict[str, Any],\n",
        "    is_diverging: bool = False\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Generates a professional, publication-quality heatmap from a matrix.\n",
        "\n",
        "    This function creates a heatmap visualization of an N x N matrix, with\n",
        "    features designed for maximum clarity and analytical insight:\n",
        "    -   Intelligently selects a sequential or diverging colormap.\n",
        "    -   Clamps the color bar range to percentiles to prevent outliers from\n",
        "        dominating the color scale.\n",
        "    -   Annotates each cell with its numerical value, ensuring text is readable\n",
        "        against any background color.\n",
        "    -   All styling (DPI, fonts, colors) is driven by the configuration dict.\n",
        "\n",
        "    Args:\n",
        "        matrix_df (pd.DataFrame): The N x N matrix to plot. Index and columns\n",
        "                                  should be asset names.\n",
        "        title (str): The title for the heatmap plot.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary.\n",
        "        is_diverging (bool): If True, uses a diverging colormap centered at 0\n",
        "                             (for data like KM coefficients). If False, uses a\n",
        "                             sequential colormap (for data like TE).\n",
        "\n",
        "    Returns:\n",
        "        plt.Figure: The matplotlib Figure object containing the heatmap.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(matrix_df, pd.DataFrame) or matrix_df.shape[0] != matrix_df.shape[1]:\n",
        "        raise TypeError(\"Input 'matrix_df' must be a square pandas DataFrame.\")\n",
        "\n",
        "    # Extract visualization parameters from the configuration.\n",
        "    viz_params = validated_config['visualization_params']\n",
        "\n",
        "    # Create the figure and axes object.\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(10, 8),\n",
        "        dpi=viz_params['general']['figure_dpi']\n",
        "    )\n",
        "\n",
        "    # Determine colormap and color bar normalization based on data type.\n",
        "    if is_diverging:\n",
        "        # For diverging data (e.g., KM matrix with +/- values), center the colormap at zero.\n",
        "        cmap = viz_params['heatmaps']['colormap_diverging']\n",
        "        # Find the maximum absolute value to create a symmetric color scale.\n",
        "        abs_max = np.abs(matrix_df.values).max()\n",
        "        vmin, vmax = -abs_max, abs_max\n",
        "    else:\n",
        "        # For sequential data (e.g., TE matrix with only positive values), use a sequential map.\n",
        "        cmap = viz_params['heatmaps']['colormap_sequential']\n",
        "        # Use percentiles to prevent extreme outliers from skewing the color bar.\n",
        "        vmin = np.percentile(matrix_df.values, 5)\n",
        "        vmax = np.percentile(matrix_df.values, 95)\n",
        "\n",
        "    # Generate the heatmap using seaborn.\n",
        "    sns.heatmap(\n",
        "        matrix_df,\n",
        "        ax=ax,\n",
        "        annot=True,          # Annotate cells with their values.\n",
        "        fmt=\".3f\",           # Format annotations to 3 decimal places.\n",
        "        cmap=cmap,           # Apply the selected colormap.\n",
        "        linewidths=.5,       # Add lines between cells.\n",
        "        vmin=vmin,           # Set the min value for the color bar.\n",
        "        vmax=vmax,           # Set the max value for the color bar.\n",
        "        cbar_kws={'label': 'Magnitude'} # Add a label to the color bar.\n",
        "    )\n",
        "\n",
        "    # Set plot title and labels.\n",
        "    ax.set_title(title, fontsize=viz_params['general']['font_size'] + 4, pad=20)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.tick_params(axis='y', rotation=0)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_network_graph(\n",
        "    matrix_df: pd.DataFrame,\n",
        "    title: str,\n",
        "    validated_config: Dict[str, Any],\n",
        "    edge_threshold_percentile: float = 75.0\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Generates a directed network graph from an adjacency matrix.\n",
        "\n",
        "    This function visualizes the relationships in an N x N matrix as a directed\n",
        "    graph, with features designed for analytical clarity:\n",
        "    -   Creates a directed graph where nodes are assets and edges represent\n",
        "        interactions (e.g., information flow, drift influence).\n",
        "    -   Filters edges based on a percentile threshold to reduce clutter and\n",
        "        focus on the most significant connections.\n",
        "    -   Encodes information visually: edge width maps to interaction strength,\n",
        "        and edge color can represent the nature of the link (positive/negative).\n",
        "    -   Node size is proportional to its influence (weighted degree).\n",
        "\n",
        "    Args:\n",
        "        matrix_df (pd.DataFrame): The N x N adjacency matrix. For a directed\n",
        "                                  graph, matrix[i, j] represents the edge\n",
        "                                  from node j to node i.\n",
        "        title (str): The title for the network plot.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary.\n",
        "        edge_threshold_percentile (float): The percentile of edge weights above\n",
        "                                           which edges will be drawn. E.g., 75.0\n",
        "                                           means only the top 25% of edges are shown.\n",
        "\n",
        "    Returns:\n",
        "        plt.Figure: The matplotlib Figure object containing the network graph.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(matrix_df, pd.DataFrame) or matrix_df.shape[0] != matrix_df.shape[1]:\n",
        "        raise TypeError(\"Input 'matrix_df' must be a square pandas DataFrame.\")\n",
        "\n",
        "    # Extract visualization parameters.\n",
        "    viz_params = validated_config['visualization_params']\n",
        "    net_params = viz_params['network_graphs']\n",
        "\n",
        "    # Create a directed graph from the pandas DataFrame.\n",
        "    # Note: nx.from_pandas_adjacency creates an edge from col `j` to row `i`.\n",
        "    G = nx.from_pandas_adjacency(matrix_df, create_using=nx.DiGraph())\n",
        "\n",
        "    # --- Edge Filtering and Styling ---\n",
        "    # Get all edge weights and determine the threshold for filtering.\n",
        "    all_weights = [abs(G.edges[edge]['weight']) for edge in G.edges()]\n",
        "    if not all_weights:\n",
        "        print(f\"Warning: No edges in the graph for '{title}'. Cannot plot.\")\n",
        "        # Return an empty figure if there are no edges.\n",
        "        fig, ax = plt.subplots(figsize=(10, 10), dpi=viz_params['general']['figure_dpi'])\n",
        "        ax.set_title(title)\n",
        "        ax.text(0.5, 0.5, \"No significant connections to display.\", ha='center', va='center')\n",
        "        return fig\n",
        "\n",
        "    threshold = np.percentile(all_weights, edge_threshold_percentile)\n",
        "\n",
        "    # Prepare lists for edges and their visual properties.\n",
        "    edges_to_draw = []\n",
        "    edge_widths = []\n",
        "    edge_colors = []\n",
        "\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        weight = data['weight']\n",
        "        # Only include edges that are above the significance threshold.\n",
        "        if abs(weight) >= threshold:\n",
        "            edges_to_draw.append((u, v))\n",
        "            # Scale edge width by weight magnitude.\n",
        "            edge_widths.append(abs(weight) * net_params['edge_width_multiplier'])\n",
        "            # Color edges based on sign (for KM) or use a default (for TE).\n",
        "            edge_colors.append('royalblue' if weight > 0 else 'firebrick')\n",
        "\n",
        "    # --- Node Styling ---\n",
        "    # Calculate node size based on weighted degree (a measure of influence).\n",
        "    # Use the filtered edges for the calculation.\n",
        "    temp_graph = nx.DiGraph()\n",
        "    temp_graph.add_weighted_edges_from([(u, v, w) for (u,v),w in zip(edges_to_draw, edge_widths)])\n",
        "    degrees = dict(temp_graph.degree(weight='weight'))\n",
        "    node_sizes = [degrees.get(node, 0) * 50 + net_params['node_size'] for node in G.nodes()]\n",
        "\n",
        "    # --- Drawing ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 10), dpi=viz_params['general']['figure_dpi'])\n",
        "\n",
        "    # Use the specified layout function (e.g., circular).\n",
        "    pos = getattr(nx, net_params['layout_function'])(G)\n",
        "\n",
        "    # Draw the network components.\n",
        "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=node_sizes, node_color=net_params['node_color'])\n",
        "    nx.draw_networkx_labels(G, pos, ax=ax, font_size=viz_params['general']['font_size'], font_weight='bold')\n",
        "    nx.draw_networkx_edges(\n",
        "        G, pos, ax=ax,\n",
        "        edgelist=edges_to_draw,\n",
        "        width=edge_widths,\n",
        "        edge_color=edge_colors,\n",
        "        arrowstyle='-|>',\n",
        "        arrows=True,\n",
        "        arrowsize=net_params['arrow_size'],\n",
        "        node_size=node_sizes, # Helps with arrow positioning.\n",
        "        connectionstyle='arc3,rad=0.1' # Use curved edges to see bidirectional flows.\n",
        "    )\n",
        "\n",
        "    ax.set_title(title, fontsize=viz_params['general']['font_size'] + 6)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "1SXMmau20wGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Results Interpretation\n",
        "\n",
        "def _unstack_results_to_dataframe(\n",
        "    time_resolved_results: Dict[pd.Timestamp, pd.DataFrame],\n",
        "    analysis_type: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforms a dictionary of time-stamped matrices into a 2D DataFrame.\n",
        "\n",
        "    This helper function \"unstacks\" or \"melts\" the 3D time-resolved data\n",
        "    (time, source, target) into a 2D DataFrame where the index is time and\n",
        "    the columns represent each unique network link (source, target). This\n",
        "    format is ideal for performing time series analysis on individual links.\n",
        "\n",
        "    Args:\n",
        "        time_resolved_results (Dict[pd.Timestamp, pd.DataFrame]):\n",
        "            A dictionary where keys are timestamps and values are N x N matrices.\n",
        "        analysis_type (str): A string label for the analysis type (e.g., 'TE').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a DatetimeIndex and a MultiIndex for\n",
        "                      columns of the form (analysis_type, source, target).\n",
        "    \"\"\"\n",
        "    # Handle the case of empty results.\n",
        "    if not time_resolved_results:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Use a list comprehension to build a list of records for DataFrame creation.\n",
        "    # This is more efficient than appending to a DataFrame in a loop.\n",
        "    records = []\n",
        "    for timestamp, matrix in time_resolved_results.items():\n",
        "        # Skip any windows where the calculation might have failed.\n",
        "        if matrix is None:\n",
        "            continue\n",
        "        # Stack the matrix to turn it into a Series with a MultiIndex (target, source).\n",
        "        record = matrix.stack()\n",
        "        # Add the timestamp to the record.\n",
        "        record.name = timestamp\n",
        "        records.append(record)\n",
        "\n",
        "    # Concatenate all records into a single DataFrame.\n",
        "    if not records:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    unstacked_df = pd.concat(records, axis=1).T\n",
        "    # Create a new MultiIndex for the columns for clarity.\n",
        "    unstacked_df.columns = pd.MultiIndex.from_tuples(\n",
        "        [(analysis_type, target, source) for target, source in unstacked_df.columns],\n",
        "        names=['analysis', 'target', 'source']\n",
        "    )\n",
        "    return unstacked_df\n",
        "\n",
        "def interpret_dynamic_results(\n",
        "    time_resolved_results: Dict[str, Dict[pd.Timestamp, pd.DataFrame]],\n",
        "    magnitude_z_score_threshold: float = 2.0,\n",
        "    consistency_quantile_threshold: float = 0.75,\n",
        "    short_window_roll: int = 6,\n",
        "    long_window_roll: int = 24,\n",
        "    shift_z_score_threshold: float = 3.0\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Automates interpretation of time-resolved network analysis results.\n",
        "\n",
        "    This function provides a two-pronged interpretation of dynamic network data:\n",
        "    1.  Persistent Interaction Identification: It identifies network links that\n",
        "        are both strong on average (high magnitude) and consistently present\n",
        "        over time (high consistency).\n",
        "    2.  Regime Shift Detection: It screens for structural breaks by identifying\n",
        "        moments where a link's short-term behavior deviates significantly\n",
        "        from its long-term historical norm, using a rolling z-score method.\n",
        "\n",
        "    Args:\n",
        "        time_resolved_results (Dict[str, Dict[pd.Timestamp, pd.DataFrame]]):\n",
        "            The output from `perform_time_resolved_analysis`.\n",
        "        magnitude_z_score_threshold (float): Z-score threshold for a link's\n",
        "            average strength to be considered high magnitude.\n",
        "        consistency_quantile_threshold (float): Quantile of a link's own\n",
        "            history used as a threshold for the consistency calculation.\n",
        "        short_window_roll (int): The lookback period for the short-term\n",
        "            rolling mean in regime shift detection.\n",
        "        long_window_roll (int): The lookback period for the long-term\n",
        "            rolling mean and standard deviation in regime shift detection.\n",
        "        shift_z_score_threshold (float): The z-score threshold for flagging a\n",
        "            potential regime shift.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing DataFrames and dictionaries that\n",
        "                        summarize the interpretation for both TE and KM analyses.\n",
        "                        Includes persistent links and detected regime shifts.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(time_resolved_results, dict):\n",
        "        raise TypeError(\"time_resolved_results must be a dictionary.\")\n",
        "\n",
        "    print(\"Interpreting dynamic results for persistent interactions and regime shifts...\")\n",
        "    interpretation_summary = {}\n",
        "    epsilon = 1e-15 # For numerical stability\n",
        "\n",
        "    # Iterate over each analysis type (TE and KM).\n",
        "    for analysis_type, results_dict in time_resolved_results.items():\n",
        "        # --- Step 1: Data Transformation ---\n",
        "        links_df = _unstack_results_to_dataframe(results_dict, analysis_type)\n",
        "\n",
        "        if links_df.empty:\n",
        "            print(f\"  No data to interpret for {analysis_type}.\")\n",
        "            interpretation_summary[analysis_type] = {\n",
        "                'all_links_summary': pd.DataFrame(),\n",
        "                'persistent_links': pd.DataFrame(),\n",
        "                'regime_shifts': {}\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        # --- Step 2: Persistent Interaction Identification ---\n",
        "        # (This section is retained from the original implementation)\n",
        "        if analysis_type == 'kramers_moyal':\n",
        "            mean_strengths = links_df.abs().mean(axis=0)\n",
        "        else:\n",
        "            mean_strengths = links_df.mean(axis=0)\n",
        "\n",
        "        grand_mean = mean_strengths.mean()\n",
        "        grand_std = mean_strengths.std()\n",
        "        magnitude_z_scores = (mean_strengths - grand_mean) / grand_std\n",
        "\n",
        "        def consistency_score(series: pd.Series) -> float:\n",
        "            threshold = series.quantile(consistency_quantile_threshold)\n",
        "            return (series > threshold).mean() if not series.empty else 0.0\n",
        "\n",
        "        consistency_scores = links_df.apply(consistency_score, axis=0)\n",
        "\n",
        "        summary_df = pd.DataFrame({\n",
        "            'Mean Strength': mean_strengths,\n",
        "            'Magnitude Z-Score': magnitude_z_scores,\n",
        "            'Consistency Score': consistency_scores\n",
        "        })\n",
        "\n",
        "        is_persistent = (summary_df['Magnitude Z-Score'] > magnitude_z_score_threshold) & \\\n",
        "                        (summary_df['Consistency Score'] > consistency_quantile_threshold)\n",
        "        summary_df['Is Persistent'] = is_persistent\n",
        "        persistent_links_df = summary_df[summary_df['Is Persistent']].sort_values(\n",
        "            by='Magnitude Z-Score', ascending=False\n",
        "        )\n",
        "        print(f\"  Identified {len(persistent_links_df)} persistent interactions for {analysis_type}.\")\n",
        "\n",
        "        # --- Step 3: Implement Regime Shift Detection Logic ---\n",
        "        print(f\"  Detecting regime shifts for {analysis_type}...\")\n",
        "        # Calculate short-term and long-term rolling means.\n",
        "        short_rolling_mean = links_df.rolling(window=short_window_roll, min_periods=short_window_roll).mean()\n",
        "        long_rolling_mean = links_df.rolling(window=long_window_roll, min_periods=long_window_roll).mean()\n",
        "\n",
        "        # Calculate the rolling standard deviation of the difference.\n",
        "        rolling_std = (short_rolling_mean - long_rolling_mean).rolling(window=long_window_roll).std()\n",
        "\n",
        "        # Calculate the rolling z-score of the deviation.\n",
        "        z_score_df = (short_rolling_mean - long_rolling_mean) / (rolling_std + epsilon)\n",
        "\n",
        "        # Identify points where the z-score exceeds the threshold.\n",
        "        is_shift = z_score_df.abs() > shift_z_score_threshold\n",
        "\n",
        "        # --- Step 4: Structure and Store the Regime Shift Results ---\n",
        "        # Convert the sparse boolean DataFrame into a more useful dictionary format.\n",
        "        regime_shifts = {}\n",
        "        # Get timestamps where at least one shift occurred.\n",
        "        shift_timestamps = is_shift.index[is_shift.any(axis=1)]\n",
        "\n",
        "        for ts in shift_timestamps:\n",
        "            # Get the links (columns) that shifted at this timestamp.\n",
        "            shifted_links = is_shift.columns[is_shift.loc[ts]].tolist()\n",
        "            # Store the list of shifted links for this timestamp.\n",
        "            regime_shifts[ts] = shifted_links\n",
        "\n",
        "        print(f\"  Identified {len(regime_shifts)} timestamps with potential regime shifts.\")\n",
        "\n",
        "        # --- Step 5: Update the Return Value ---\n",
        "        # Store all interpretation results for the current analysis type.\n",
        "        interpretation_summary[analysis_type] = {\n",
        "            'all_links_summary': summary_df,\n",
        "            'persistent_links': persistent_links_df,\n",
        "            'regime_shifts': regime_shifts\n",
        "        }\n",
        "\n",
        "    print(\"\\nResults interpretation complete.\")\n",
        "    return interpretation_summary\n"
      ],
      "metadata": {
        "id": "PIB0GmSe101G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Robustness Checks\n",
        "\n",
        "def run_full_analysis_pipeline(\n",
        "    raw_price_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete, end-to-end financial network analysis pipeline.\n",
        "\n",
        "    This master orchestrator function serves as the main entry point for a\n",
        "    single, comprehensive analysis run. It sequentially executes all the\n",
        "    required analytical steps, from data validation and preprocessing to\n",
        "    dynamic analysis and interpretation, ensuring a rigorous and reproducible\n",
        "    workflow.\n",
        "\n",
        "    The pipeline proceeds as follows:\n",
        "    1.  Validates all inputs (data and configuration).\n",
        "    2.  Preprocesses raw prices into clean log-returns.\n",
        "    3.  Performs stationarity checks on prices and returns.\n",
        "    4.  Generates descriptive statistics and visualizations.\n",
        "    5.  Calculates static (full-period) TE and KM matrices.\n",
        "    6.  Executes the time-resolved (sliding window) analysis for TE and KM.\n",
        "    7.  Analyzes the impact of pre-defined crisis periods.\n",
        "    8.  Interprets the dynamic results to identify persistent interactions.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw, datetime-indexed DataFrame of\n",
        "                                     asset closing prices.\n",
        "        study_config (Dict[str, Any]): The configuration dictionary defining\n",
        "                                       all parameters for the study.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all artifacts\n",
        "                        generated during the analysis, including processed data,\n",
        "                        statistical results, static and dynamic matrices,\n",
        "                        crisis analysis, interpretations, and figures.\n",
        "    \"\"\"\n",
        "    # Initialize the main dictionary to store all results.\n",
        "    results_bundle = {}\n",
        "    print(\"=\"*80)\n",
        "    print(\"STARTING FULL ANALYSIS PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # --- Task 1: Parameter and Data Validation ---\n",
        "        validated_config = validate_inputs(raw_price_df, study_config)\n",
        "        results_bundle['validated_config'] = validated_config\n",
        "\n",
        "        # --- Task 2: Data Preprocessing ---\n",
        "        log_return_df, preprocessing_metadata = preprocess_price_data(\n",
        "            raw_price_df, validated_config\n",
        "        )\n",
        "        results_bundle['preprocessed_data'] = {\n",
        "            'log_returns': log_return_df,\n",
        "            'metadata': preprocessing_metadata\n",
        "        }\n",
        "\n",
        "        # --- Task 3: Stationarity Check ---\n",
        "        # We need the price data aligned with the log-return data for a fair comparison.\n",
        "        aligned_price_df = raw_price_df.loc[log_return_df.index]\n",
        "        stationarity_results = perform_stationarity_analysis(\n",
        "            aligned_price_df, log_return_df\n",
        "        )\n",
        "        results_bundle['stationarity_analysis'] = stationarity_results\n",
        "\n",
        "        # --- Task 4: Descriptive Statistics ---\n",
        "        stats_df, returns_fig = generate_descriptive_statistics(\n",
        "            log_return_df, validated_config\n",
        "        )\n",
        "        results_bundle['descriptive_statistics'] = {\n",
        "            'statistics_table': stats_df,\n",
        "            'log_returns_figure': returns_fig\n",
        "        }\n",
        "\n",
        "        # --- Task 5 & 6: Static (Full-Period) Analysis ---\n",
        "        print(\"\\n--- Performing Static (Full-Period) Analysis ---\")\n",
        "        static_te_matrix = calculate_transfer_entropy(log_return_df, validated_config)\n",
        "        static_km_matrix = calculate_kramers_moyal_drift_matrix(log_return_df, validated_config)\n",
        "        results_bundle['static_analysis'] = {\n",
        "            'transfer_entropy_matrix': static_te_matrix,\n",
        "            'kramers_moyal_matrix': static_km_matrix\n",
        "        }\n",
        "\n",
        "        # --- Task 7: Time-Resolved Analysis ---\n",
        "        time_resolved_results = perform_time_resolved_analysis(\n",
        "            log_return_df, validated_config\n",
        "        )\n",
        "        results_bundle['time_resolved_analysis'] = time_resolved_results\n",
        "\n",
        "        # --- Task 8: Crisis Period Analysis ---\n",
        "        crisis_analysis_results = analyze_crisis_periods(\n",
        "            time_resolved_results, validated_config\n",
        "        )\n",
        "        results_bundle['crisis_period_analysis'] = crisis_analysis_results\n",
        "\n",
        "        # --- Task 9: Network Visualization (on static results for demonstration) ---\n",
        "        # Visualizations are generated but stored in a dedicated key.\n",
        "        print(\"\\n--- Generating Visualizations for Static Analysis ---\")\n",
        "        static_te_heatmap = plot_matrix_heatmap(\n",
        "            static_te_matrix, \"Static Transfer Entropy\", validated_config, is_diverging=False\n",
        "        )\n",
        "        static_km_heatmap = plot_matrix_heatmap(\n",
        "            static_km_matrix, \"Static Kramers-Moyal Drift\", validated_config, is_diverging=True\n",
        "        )\n",
        "        static_te_graph = plot_network_graph(\n",
        "            static_te_matrix, \"Static TE Network (Top 25% Links)\", validated_config\n",
        "        )\n",
        "        static_km_graph = plot_network_graph(\n",
        "            static_km_matrix, \"Static KM Network (Top 25% Links)\", validated_config\n",
        "        )\n",
        "        results_bundle['visualizations'] = {\n",
        "            'static_te_heatmap': static_te_heatmap,\n",
        "            'static_km_heatmap': static_km_heatmap,\n",
        "            'static_te_graph': static_te_graph,\n",
        "            'static_km_graph': static_km_graph\n",
        "        }\n",
        "\n",
        "        # --- Task 10: Results Interpretation ---\n",
        "        interpretation_results = interpret_dynamic_results(time_resolved_results)\n",
        "        results_bundle['interpretation'] = interpretation_results\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"FULL ANALYSIS PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception from the pipeline, print it, and re-raise.\n",
        "        print(\"\\n\" + \"!\"*80)\n",
        "        print(f\"AN ERROR OCCURRED IN THE ANALYSIS PIPELINE: {e}\")\n",
        "        print(\"!\"*80)\n",
        "        # Re-raise the exception to halt execution and allow for debugging.\n",
        "        raise\n",
        "\n",
        "    # Return the comprehensive bundle of all results.\n",
        "    return results_bundle\n",
        "\n",
        "def perform_robustness_analysis(\n",
        "    raw_price_df: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    param_grid: Dict[str, List[Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a multi-faceted robustness analysis by running the full pipeline\n",
        "    across a grid of hyperparameters.\n",
        "\n",
        "    This high-level function assesses the stability of key analytical findings,\n",
        "    including both persistent network links and detected regime shifts. It\n",
        "    systematically varies crucial parameters, quantifies how often each key\n",
        "    finding appears, and reports a \"Robustness Score\" for each.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw, datetime-indexed DataFrame of\n",
        "                                     asset closing prices.\n",
        "        base_config (Dict[str, Any]): The baseline configuration dictionary.\n",
        "        param_grid (Dict[str, List[Any]]): A dictionary where keys are parameter\n",
        "            paths (e.g., 'analysis_params.information_theory.discretization_bins')\n",
        "            and values are lists of values to test for that parameter.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the robustness summary, including\n",
        "                        DataFrames that list persistent links and regime shifts\n",
        "                        and the percentage of runs in which they were identified.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"STARTING COMPREHENSIVE ROBUSTNESS ANALYSIS\")\n",
        "    print(\"WARNING: This process is computationally intensive and may take a long time.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- Step 1: Generate all parameter combinations ---\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    run_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "    num_runs = len(run_combinations)\n",
        "    print(f\"Will perform {num_runs} full analysis runs with different parameter configurations.\")\n",
        "\n",
        "    # Initialize lists to store the key findings from each run.\n",
        "    all_persistent_te_links = []\n",
        "    all_persistent_km_links = []\n",
        "    all_te_regime_shifts = []\n",
        "    all_km_regime_shifts = []\n",
        "\n",
        "    # --- Step 2: Iterate through combinations and run the pipeline ---\n",
        "    for i, params in enumerate(run_combinations):\n",
        "        print(f\"\\n--- Running Analysis {i+1}/{num_runs} with params: {params} ---\")\n",
        "        run_config = deepcopy(base_config)\n",
        "\n",
        "        for key_path, value in params.items():\n",
        "            keys_list = key_path.split('.')\n",
        "            d = run_config\n",
        "            for key in keys_list[:-1]:\n",
        "                d = d[key]\n",
        "            d[keys_list[-1]] = value\n",
        "\n",
        "        try:\n",
        "            results_bundle = run_full_analysis_pipeline(raw_price_df, run_config)\n",
        "\n",
        "            # --- Step 1 (cont.): Update Data Aggregation Logic ---\n",
        "            # Extract persistent links\n",
        "            te_links = results_bundle['interpretation']['transfer_entropy']['persistent_links']\n",
        "            km_links = results_bundle['interpretation']['kramers_moyal']['persistent_links']\n",
        "            all_persistent_te_links.extend(list(te_links.index))\n",
        "            all_persistent_km_links.extend(list(km_links.index))\n",
        "\n",
        "            # Extract and flatten regime shifts\n",
        "            te_shifts = results_bundle['interpretation']['transfer_entropy']['regime_shifts']\n",
        "            for ts, links in te_shifts.items():\n",
        "                for link_tuple in links:\n",
        "                    # link_tuple is ('analysis', 'target', 'source')\n",
        "                    all_te_regime_shifts.append((ts, link_tuple[1], link_tuple[2]))\n",
        "\n",
        "            km_shifts = results_bundle['interpretation']['kramers_moyal']['regime_shifts']\n",
        "            for ts, links in km_shifts.items():\n",
        "                for link_tuple in links:\n",
        "                    all_km_regime_shifts.append((ts, link_tuple[1], link_tuple[2]))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  WARNING: Run {i+1} failed with error: {e}. Skipping this configuration.\")\n",
        "            continue\n",
        "\n",
        "    # --- Step 3: Aggregate and Summarize Robustness ---\n",
        "    print(\"\\n--- Aggregating Robustness Results ---\")\n",
        "\n",
        "    # Helper function to create robustness summary DataFrames.\n",
        "    def create_robustness_df(link_counts: Counter, total_runs: int, index_names: List[str]) -> pd.DataFrame:\n",
        "        if not link_counts:\n",
        "            return pd.DataFrame(columns=['Count', 'Robustness Score (%)'])\n",
        "\n",
        "        df = pd.DataFrame.from_dict(link_counts, orient='index', columns=['Count'])\n",
        "        df['Robustness Score (%)'] = (df['Count'] / total_runs) * 100\n",
        "        df.index = pd.MultiIndex.from_tuples(df.index, names=index_names)\n",
        "        return df.sort_values(by='Robustness Score (%)', ascending=False)\n",
        "\n",
        "    # Process persistent links\n",
        "    te_persistent_counts = Counter(all_persistent_te_links)\n",
        "    km_persistent_counts = Counter(all_persistent_km_links)\n",
        "    te_persistent_robustness_df = create_robustness_df(te_persistent_counts, num_runs, ['analysis', 'target', 'source'])\n",
        "    km_persistent_robustness_df = create_robustness_df(km_persistent_counts, num_runs, ['analysis', 'target', 'source'])\n",
        "\n",
        "    # Process regime shifts\n",
        "    te_shift_counts = Counter(all_te_regime_shifts)\n",
        "    km_shift_counts = Counter(all_km_regime_shifts)\n",
        "    te_shift_robustness_df = create_robustness_df(te_shift_counts, num_runs, ['timestamp', 'target', 'source'])\n",
        "    km_shift_robustness_df = create_robustness_df(km_shift_counts, num_runs, ['timestamp', 'target', 'source'])\n",
        "\n",
        "    # Assemble the final, comprehensive robustness summary.\n",
        "    robustness_summary = {\n",
        "        'run_configurations': run_combinations,\n",
        "        'total_runs': num_runs,\n",
        "        'persistent_links': {\n",
        "            'transfer_entropy': te_persistent_robustness_df,\n",
        "            'kramers_moyal': km_persistent_robustness_df\n",
        "        },\n",
        "        'regime_shifts': {\n",
        "            'transfer_entropy': te_shift_robustness_df,\n",
        "            'kramers_moyal': km_shift_robustness_df\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ROBUSTNESS ANALYSIS COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return robustness_summary\n"
      ],
      "metadata": {
        "id": "4BUNpBb54qax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Error Analysis\n",
        "\n",
        "def _block_bootstrap_generator(\n",
        "    df: pd.DataFrame,\n",
        "    num_samples: int\n",
        ") -> Generator[pd.DataFrame, None, None]:\n",
        "    \"\"\"\n",
        "    A memory-efficient generator for block bootstrap resampling of time series.\n",
        "\n",
        "    This generator creates bootstrap samples by sampling blocks of consecutive\n",
        "    rows with replacement. This method preserves the autocorrelation structure\n",
        "    inherent in time series data, which would be destroyed by simple i.i.d.\n",
        "    resampling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The time series DataFrame to be resampled.\n",
        "        num_samples (int): The total number of bootstrap samples to generate.\n",
        "\n",
        "    Yields:\n",
        "        Generator[pd.DataFrame, None, None]: A generator that yields a new\n",
        "            bootstrapped DataFrame in each iteration.\n",
        "    \"\"\"\n",
        "    # Get the total number of observations and define the block size.\n",
        "    n_obs = len(df)\n",
        "    # A common heuristic for block size is the square root of the series length.\n",
        "    block_size = int(np.sqrt(n_obs))\n",
        "\n",
        "    # Calculate the number of blocks needed to approximate the original length.\n",
        "    num_blocks = int(np.ceil(n_obs / block_size))\n",
        "    # Define the possible starting indices for the blocks.\n",
        "    possible_starts = np.arange(0, n_obs - block_size + 1)\n",
        "\n",
        "    # Generate the requested number of bootstrap samples.\n",
        "    for _ in range(num_samples):\n",
        "        # Randomly sample the starting indices of the blocks with replacement.\n",
        "        block_start_indices = np.random.choice(possible_starts, size=num_blocks, replace=True)\n",
        "\n",
        "        # Create the new series by concatenating the chosen blocks.\n",
        "        # This is done efficiently using a list comprehension and pd.concat.\n",
        "        bootstrap_sample_df = pd.concat(\n",
        "            [df.iloc[i : i + block_size] for i in block_start_indices],\n",
        "            axis=0\n",
        "        )\n",
        "\n",
        "        # Truncate the sample to the original length and reset the index.\n",
        "        # The index is reset because the concatenated index is not meaningful.\n",
        "        yield bootstrap_sample_df.iloc[:n_obs].reset_index(drop=True)\n",
        "\n",
        "def perform_error_analysis(\n",
        "    log_return_df: pd.DataFrame,\n",
        "    validated_config: Dict[str, Any],\n",
        "    num_samples: int = 500,\n",
        "    alpha: float = 0.05\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs error analysis using block bootstrapping to estimate confidence intervals.\n",
        "\n",
        "    This function quantifies the uncertainty around the static TE and KM matrix\n",
        "    estimates. It generates numerous resampled time series using a block\n",
        "    bootstrap method (to preserve autocorrelation) and re-calculates the\n",
        "    matrices for each sample. The distribution of these bootstrapped results\n",
        "    is then used to construct confidence intervals for each coefficient.\n",
        "\n",
        "    Args:\n",
        "        log_return_df (pd.DataFrame): The clean, stationary log-return DataFrame.\n",
        "        validated_config (Dict[str, Any]): The validated configuration dictionary.\n",
        "        num_samples (int): The number of bootstrap samples to generate.\n",
        "                           Warning: High numbers are computationally expensive.\n",
        "        alpha (float): The significance level. E.g., 0.05 for a 95% CI.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the error analysis results, with\n",
        "                        keys 'transfer_entropy' and 'kramers_moyal'. Each value\n",
        "                        is another dictionary containing:\n",
        "                        - 'point_estimate': The matrix calculated on original data.\n",
        "                        - 'lower_ci': The matrix of lower bounds of the CI.\n",
        "                        - 'upper_ci': The matrix of upper bounds of the CI.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"STARTING ERROR ANALYSIS (BOOTSTRAPPING)\")\n",
        "    print(f\"WARNING: This is computationally intensive. Running {num_samples} bootstrap samples.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- Calculate the original point estimates first ---\n",
        "    print(\"Calculating original point estimates...\")\n",
        "    point_estimate_te = calculate_transfer_entropy(log_return_df, validated_config)\n",
        "    point_estimate_km = calculate_kramers_moyal_drift_matrix(log_return_df, validated_config)\n",
        "\n",
        "    # Initialize lists to store the results from each bootstrap iteration.\n",
        "    bootstrap_te_results = []\n",
        "    bootstrap_km_results = []\n",
        "\n",
        "    # --- Create and iterate through the bootstrap samples ---\n",
        "    bootstrap_generator = _block_bootstrap_generator(log_return_df, num_samples)\n",
        "\n",
        "    for i, resampled_df in tqdm(enumerate(bootstrap_generator), total=num_samples, desc=\"Bootstrap Iterations\"):\n",
        "        # The resampled df has a generic integer index, which is fine for the\n",
        "        # calculation functions as they don't rely on the datetime values.\n",
        "\n",
        "        # --- Calculate TE and KM on the resampled data ---\n",
        "        try:\n",
        "            # Calculate TE matrix for the current bootstrap sample.\n",
        "            te_matrix = calculate_transfer_entropy(resampled_df, validated_config)\n",
        "            bootstrap_te_results.append(te_matrix.values)\n",
        "        except Exception as e:\n",
        "            # If a calculation fails, print a warning and skip this iteration.\n",
        "            print(f\"\\nWarning: TE calculation failed on bootstrap sample {i+1}: {e}\")\n",
        "\n",
        "        try:\n",
        "            # Calculate KM matrix for the current bootstrap sample.\n",
        "            km_matrix = calculate_kramers_moyal_drift_matrix(resampled_df, validated_config)\n",
        "            bootstrap_km_results.append(km_matrix.values)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nWarning: KM calculation failed on bootstrap sample {i+1}: {e}\")\n",
        "\n",
        "    # --- Calculate Confidence Intervals from the bootstrap distributions ---\n",
        "    print(\"\\nCalculating confidence intervals from bootstrap distributions...\")\n",
        "\n",
        "    # Define the lower and upper percentile bounds for the CI.\n",
        "    lower_percentile = (alpha / 2.0) * 100\n",
        "    upper_percentile = (1.0 - (alpha / 2.0)) * 100\n",
        "\n",
        "    # Get asset names for labeling the final DataFrames.\n",
        "    asset_names = log_return_df.columns\n",
        "\n",
        "    # --- Process TE results ---\n",
        "    te_stack = np.stack(bootstrap_te_results, axis=0)\n",
        "    te_lower_ci = np.percentile(te_stack, lower_percentile, axis=0)\n",
        "    te_upper_ci = np.percentile(te_stack, upper_percentile, axis=0)\n",
        "\n",
        "    # --- Process KM results ---\n",
        "    km_stack = np.stack(bootstrap_km_results, axis=0)\n",
        "    km_lower_ci = np.percentile(km_stack, lower_percentile, axis=0)\n",
        "    km_upper_ci = np.percentile(km_stack, upper_percentile, axis=0)\n",
        "\n",
        "    # --- Assemble the final results dictionary ---\n",
        "    error_analysis_results = {\n",
        "        'transfer_entropy': {\n",
        "            'point_estimate': point_estimate_te,\n",
        "            'lower_ci': pd.DataFrame(te_lower_ci, index=asset_names, columns=asset_names),\n",
        "            'upper_ci': pd.DataFrame(te_upper_ci, index=asset_names, columns=asset_names)\n",
        "        },\n",
        "        'kramers_moyal': {\n",
        "            'point_estimate': point_estimate_km,\n",
        "            'lower_ci': pd.DataFrame(km_lower_ci, index=asset_names, columns=asset_names),\n",
        "            'upper_ci': pd.DataFrame(km_upper_ci, index=asset_names, columns=asset_names)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ERROR ANALYSIS COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return error_analysis_results\n"
      ],
      "metadata": {
        "id": "K_vgC-Cu6K8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Pipeline\n",
        "\n",
        "def _generate_report_string(master_results: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Generates a comprehensive, human-readable report from the master results bundle.\n",
        "\n",
        "    This helper function synthesizes the outputs of the entire analysis into a\n",
        "    structured Markdown string. This revised version includes new sections for\n",
        "    the regime shift analysis, providing a complete overview of both stable\n",
        "    (persistent) and unstable (shifting) network dynamics.\n",
        "\n",
        "    Args:\n",
        "        master_results (Dict[str, Any]): The master dictionary containing all\n",
        "                                         analysis results.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted Markdown string summarizing the entire analysis.\n",
        "    \"\"\"\n",
        "    # --- Report Header ---\n",
        "    report_parts = [\n",
        "        \"# Digital Twin of Market Dynamics: Analysis Report\",\n",
        "        \"This report summarizes the findings from the end-to-end analysis pipeline.\"\n",
        "    ]\n",
        "\n",
        "    # --- Methodology Section ---\n",
        "    config = master_results['main_analysis']['validated_config']\n",
        "    report_parts.append(\"\\n## 1. Methodology and Configuration\")\n",
        "    report_parts.append(f\"- **Date Range:** {config['data_params']['date_range']['start'].date()} to {config['data_params']['date_range']['end'].date()}\")\n",
        "    report_parts.append(f\"- **Sliding Window:** Size = {config['analysis_params']['sliding_window']['window_size_days']} days, Step = {config['analysis_params']['sliding_window']['step_size_days']} days\")\n",
        "    report_parts.append(f\"- **Transfer Entropy:** Bins = {config['analysis_params']['information_theory']['discretization_bins']}\")\n",
        "    report_parts.append(f\"- **Preprocessing:** {master_results['main_analysis']['preprocessed_data']['metadata']['final_observations']} observations used after cleaning.\")\n",
        "\n",
        "    # --- Key Findings Section ---\n",
        "    report_parts.append(\"\\n## 2. Key Findings: Dynamic Interpretation Summary\")\n",
        "\n",
        "    # --- 2.1 Persistent Interactions ---\n",
        "    report_parts.append(\"\\n### 2.1. Persistent Transfer Entropy Links\")\n",
        "    persistent_te = master_results['main_analysis']['interpretation']['transfer_entropy']['persistent_links']\n",
        "    if not persistent_te.empty:\n",
        "        report_parts.append(persistent_te.to_markdown(floatfmt=\".4f\"))\n",
        "    else:\n",
        "        report_parts.append(\"No persistent TE links were identified based on the specified criteria.\")\n",
        "\n",
        "    report_parts.append(\"\\n### 2.2. Persistent Kramers-Moyal Links\")\n",
        "    persistent_km = master_results['main_analysis']['interpretation']['kramers_moyal']['persistent_links']\n",
        "    if not persistent_km.empty:\n",
        "        report_parts.append(persistent_km.to_markdown(floatfmt=\".4f\"))\n",
        "    else:\n",
        "        report_parts.append(\"No persistent KM links were identified based on the specified criteria.\")\n",
        "\n",
        "    # --- 2.3 Regime Shift Events ---\n",
        "    report_parts.append(\"\\n### 2.3. Notable Regime Shift Events\")\n",
        "    report_parts.append(\"The following dates were identified as having the largest number of simultaneous link shifts, indicating potential market-wide structural breaks.\")\n",
        "\n",
        "    # Helper to format the shift summary\n",
        "    def format_shift_summary(shifts_dict: Dict, top_n: int = 5) -> str:\n",
        "        if not shifts_dict:\n",
        "            return \"No significant regime shifts were detected.\"\n",
        "        # Sort events by the number of links that shifted\n",
        "        sorted_events = sorted(shifts_dict.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "        summary_lines = []\n",
        "        for ts, links in sorted_events[:top_n]:\n",
        "            # Format links for readability: e.g., (TE, Gold, Nasdaq) -> \"Gold -> Nasdaq\"\n",
        "            formatted_links = [f\"{link[2]} -> {link[1]}\" for link in links]\n",
        "            summary_lines.append(f\"- **{ts.date()}** ({len(links)} links shifted): {', '.join(formatted_links)}\")\n",
        "        return \"\\n\".join(summary_lines)\n",
        "\n",
        "    te_shifts = master_results['main_analysis']['interpretation']['transfer_entropy']['regime_shifts']\n",
        "    report_parts.append(\"\\n**Top Transfer Entropy Shift Events:**\")\n",
        "    report_parts.append(format_shift_summary(te_shifts))\n",
        "\n",
        "    km_shifts = master_results['main_analysis']['interpretation']['kramers_moyal']['regime_shifts']\n",
        "    report_parts.append(\"\\n**Top Kramers-Moyal Shift Events:**\")\n",
        "    report_parts.append(format_shift_summary(km_shifts))\n",
        "\n",
        "    # --- Detailed Tables Section ---\n",
        "    report_parts.append(\"\\n## 3. Detailed Results Tables\")\n",
        "    report_parts.append(\"\\n### 3.1. Descriptive Statistics (Log-Returns)\")\n",
        "    report_parts.append(master_results['main_analysis']['descriptive_statistics']['statistics_table'].to_markdown(floatfmt=\".4f\"))\n",
        "    report_parts.append(\"\\n### 3.2. Stationarity Analysis (ADF Test)\")\n",
        "    report_parts.append(master_results['main_analysis']['stationarity_analysis'].to_markdown(floatfmt=\".4f\"))\n",
        "\n",
        "    # --- Robustness Analysis Section ---\n",
        "    if 'robustness_analysis' in master_results:\n",
        "        report_parts.append(\"\\n## 4. Robustness Analysis Summary\")\n",
        "\n",
        "        report_parts.append(\"\\n### 4.1. Robust Persistent Links (TE)\")\n",
        "        report_parts.append(master_results['robustness_analysis']['persistent_links']['transfer_entropy'].head(10).to_markdown(floatfmt=\".2f\"))\n",
        "\n",
        "        report_parts.append(\"\\n### 4.2. Robust Regime Shifts (TE)\")\n",
        "        report_parts.append(\"The following shift events were most consistently detected across different parameter settings.\")\n",
        "        report_parts.append(master_results['robustness_analysis']['regime_shifts']['transfer_entropy'].head(10).to_markdown(floatfmt=\".2f\"))\n",
        "\n",
        "    # --- Figures Section ---\n",
        "    report_parts.append(\"\\n## 5. Generated Figures\")\n",
        "    report_parts.append(\"The following figures are available in the 'main_analysis' -> 'visualizations' key of the results bundle.\")\n",
        "    for key in master_results['main_analysis']['visualizations'].keys():\n",
        "        report_parts.append(f\"- `{key}`\")\n",
        "\n",
        "    return \"\\n\".join(report_parts)\n",
        "\n",
        "def _figure_to_base64_str(fig: plt.Figure) -> str:\n",
        "    \"\"\"\n",
        "    Converts a matplotlib Figure object into a Base64-encoded data URI string.\n",
        "\n",
        "    This utility function renders a figure into an in-memory binary buffer,\n",
        "    encodes the binary data into Base64, and formats it as a data URI\n",
        "    that can be directly embedded into an HTML <img> tag.\n",
        "\n",
        "    Args:\n",
        "        fig (plt.Figure): The matplotlib Figure object to convert.\n",
        "\n",
        "    Returns:\n",
        "        str: A data URI string for the figure (e.g., \"data:image/png;base64,...\").\n",
        "    \"\"\"\n",
        "    # --- Step 1: Render to an In-Memory Buffer ---\n",
        "    # Create a binary buffer in memory to hold the image data.\n",
        "    buf = io.BytesIO()\n",
        "    # Save the figure to the buffer in PNG format.\n",
        "    # Using a moderate DPI is good for web display without creating huge files.\n",
        "    fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "    # The buffer's cursor is at the end, so we need to seek back to the start.\n",
        "    buf.seek(0)\n",
        "\n",
        "    # --- Step 2: Encode to Base64 ---\n",
        "    # Read the binary data from the buffer.\n",
        "    image_bytes = buf.read()\n",
        "    # Encode the binary data using the base64 standard library.\n",
        "    encoded_bytes = base64.b64encode(image_bytes)\n",
        "    # Close the buffer to free memory.\n",
        "    buf.close()\n",
        "\n",
        "    # --- Step 3: Decode to a String ---\n",
        "    # Decode the Base64 bytes into a standard UTF-8 string.\n",
        "    encoded_string = encoded_bytes.decode('utf-8')\n",
        "\n",
        "    # --- Step 4: Return the Data URI ---\n",
        "    # Format the final data URI string for use in an HTML src attribute.\n",
        "    return f\"data:image/png;base64,{encoded_string}\"\n",
        "\n",
        "def _generate_html_report(master_results: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Generates a comprehensive, self-contained, and robust HTML report.\n",
        "\n",
        "    This function synthesizes all analytical outputs into a single, dynamic HTML\n",
        "    file. This revised version includes robust checks for missing figures,\n",
        "    ensuring that the report generation never fails even if an upstream\n",
        "    visualization step encounters an error.\n",
        "\n",
        "    Args:\n",
        "        master_results (Dict[str, Any]): The master dictionary containing all\n",
        "                                         analysis results.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the full HTML source code of the report.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold the parts of the HTML document.\n",
        "    html_parts = []\n",
        "\n",
        "    # --- Step 1: Define HTML Structure and CSS Styling ---\n",
        "    # This CSS provides a clean, professional, and readable style for the report.\n",
        "    html_css = \"\"\"\n",
        "    <style>\n",
        "        body { font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; margin: 2em; max-width: 1200px; margin-left: auto; margin-right: auto; }\n",
        "        h1, h2, h3, h4 { color: #2c3e50; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px; }\n",
        "        h1 { font-size: 2.5em; text-align: center; }\n",
        "        h2 { font-size: 2em; }\n",
        "        h3 { font-size: 1.5em; }\n",
        "        p.error-msg { color: #c0392b; font-style: italic; }\n",
        "        table.styled-table { border-collapse: collapse; margin: 25px 0; font-size: 0.9em; width: 100%; box-shadow: 0 0 20px rgba(0, 0, 0, 0.15); }\n",
        "        table.styled-table thead tr { background-color: #009879; color: #ffffff; text-align: left; }\n",
        "        table.styled-table th, table.styled-table td { padding: 12px 15px; }\n",
        "        table.styled-table tbody tr { border-bottom: 1px solid #dddddd; }\n",
        "        table.styled-table tbody tr:nth-of-type(even) { background-color: #f3f3f3; }\n",
        "        table.styled-table tbody tr:last-of-type { border-bottom: 2px solid #009879; }\n",
        "        img { max-width: 100%; height: auto; display: block; margin-left: auto; margin-right: auto; border: 1px solid #ddd; border-radius: 4px; padding: 5px; margin-top: 20px; margin-bottom: 20px; }\n",
        "        details { border: 1px solid #aaa; border-radius: 4px; padding: .5em .5em 0; margin-top: 1em; }\n",
        "        summary { font-weight: bold; margin: -.5em -.5em 0; padding: .5em; cursor: pointer; background-color: #f9f9f9; }\n",
        "        details[open] { padding: .5em; background-color: #fff; }\n",
        "        details[open] summary { border-bottom: 1px solid #aaa; margin-bottom: .5em; }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    # HTML boilerplate\n",
        "    html_parts.append(\"<!DOCTYPE html><html lang='en'><head><meta charset='UTF-8'><title>Digital Twin Analysis Report</title>\")\n",
        "    html_parts.append(html_css)\n",
        "    html_parts.append(\"</head><body>\")\n",
        "    html_parts.append(\"<h1>Digital Twin of Market Dynamics: Analysis Report</h1>\")\n",
        "\n",
        "    # --- Step 2: Generate Methodology and Table Sections ---\n",
        "    config = master_results['main_analysis']['validated_config']\n",
        "    html_parts.append(\"<h2>1. Methodology and Configuration</h2>\")\n",
        "    html_parts.append(f\"<ul><li><b>Date Range:</b> {config['data_params']['date_range']['start'].date()} to {config['data_params']['date_range']['end'].date()}</li>\")\n",
        "    html_parts.append(f\"<li><b>Sliding Window:</b> Size = {config['analysis_params']['sliding_window']['window_size_days']} days, Step = {config['analysis_params']['sliding_window']['step_size_days']} days</li>\")\n",
        "    html_parts.append(f\"<li><b>Transfer Entropy:</b> Bins = {config['analysis_params']['information_theory']['discretization_bins']}</li></ul>\")\n",
        "\n",
        "    html_parts.append(\"<h2>2. Data Diagnostics</h2>\")\n",
        "    html_parts.append(\"<h3>Descriptive Statistics (Log-Returns)</h3>\")\n",
        "    stats_table = master_results['main_analysis']['descriptive_statistics']['statistics_table']\n",
        "    html_parts.append(stats_table.to_html(classes='styled-table', float_format='{:.4f}'.format))\n",
        "\n",
        "    html_parts.append(\"<h3>Stationarity Analysis (ADF Test)</h3>\")\n",
        "    stationarity_table = master_results['main_analysis']['stationarity_analysis']\n",
        "    html_parts.append(stationarity_table.to_html(classes='styled-table', float_format='{:.4f}'.format))\n",
        "\n",
        "    # --- Step 3: Generate and Embed Figures with Robust Checks ---\n",
        "    html_parts.append(\"<h2>3. Visualizations</h2>\")\n",
        "    visualizations = master_results['main_analysis'].get('visualizations', {})\n",
        "\n",
        "    # Helper function to safely embed a figure or report an error.\n",
        "    def embed_figure(fig_key: str, title: str):\n",
        "        # Use .get() for safe access to the figure object.\n",
        "        figure_object = visualizations.get(fig_key)\n",
        "        # Check if the figure object exists and is not None.\n",
        "        if figure_object is not None:\n",
        "            # If it exists, embed it.\n",
        "            html_parts.append(f\"<h4>{title}</h4>\")\n",
        "            html_parts.append(f'<img src=\"{_figure_to_base64_str(figure_object)}\" alt=\"{title}\">')\n",
        "            # Close the figure to release memory, a critical best practice.\n",
        "            plt.close(figure_object)\n",
        "        else:\n",
        "            # If it doesn't exist, report its absence gracefully.\n",
        "            html_parts.append(f\"<h4>{title}</h4>\")\n",
        "            html_parts.append(f'<p class=\"error-msg\"><i>[Figure: {title}] could not be generated due to an error in the visualization stage.</i></p>')\n",
        "\n",
        "    # Embed all key figures using the safe helper function.\n",
        "    embed_figure('log_returns_figure', 'Log-Return Time Series')\n",
        "    embed_figure('static_te_heatmap', 'Static Transfer Entropy Heatmap')\n",
        "    embed_figure('static_km_heatmap', 'Static Kramers-Moyal Drift Heatmap')\n",
        "    embed_figure('static_te_graph', 'Static TE Network (Top 25% Links)')\n",
        "    embed_figure('static_km_graph', 'Static KM Network (Top 25% Links)')\n",
        "\n",
        "    # --- Step 4: Add Interactivity for Detailed Tables ---\n",
        "    html_parts.append(\"<h2>4. Dynamic Analysis Summaries</h2>\")\n",
        "\n",
        "    # Use .get() for safe access to nested dictionaries.\n",
        "    interpretation = master_results['main_analysis'].get('interpretation', {})\n",
        "\n",
        "    html_parts.append(\"<details><summary>Click to view Persistent Interaction Analysis</summary>\")\n",
        "    persistent_te = interpretation.get('transfer_entropy', {}).get('persistent_links')\n",
        "    html_parts.append(\"<h4>Persistent Transfer Entropy Links</h4>\")\n",
        "    if persistent_te is not None and not persistent_te.empty:\n",
        "        html_parts.append(persistent_te.to_html(classes='styled-table', float_format='{:.4f}'.format))\n",
        "    else:\n",
        "        html_parts.append(\"<p>No persistent TE links were identified.</p>\")\n",
        "    html_parts.append(\"</details>\")\n",
        "\n",
        "    # --- Step 5: Assemble and Return ---\n",
        "    html_parts.append(\"</body></html>\")\n",
        "    return \"\".join(html_parts)\n",
        "\n",
        "def run_master_pipeline(\n",
        "    raw_price_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    param_grid: Dict[str, List[Any]],\n",
        "    report_format: str = 'html',\n",
        "    run_robustness_analysis: bool = True,\n",
        "    run_error_analysis: bool = True,\n",
        "    bootstrap_samples: int = 100\n",
        ") -> Tuple[Dict[str, Any], Optional[str]]:\n",
        "    \"\"\"\n",
        "    The master orchestrator for the end-to-end Digital Twin analysis pipeline.\n",
        "\n",
        "    This function serves as the single entry point for a complete analysis. It\n",
        "    sequentially executes all required analytical stages, from data validation\n",
        "    and preprocessing to dynamic analysis, interpretation, robustness checks,\n",
        "    and error analysis. Finally, it compiles all generated artifacts into a\n",
        "    comprehensive results bundle and synthesizes the findings into a user-\n",
        "    specified report format.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw, datetime-indexed DataFrame of\n",
        "                                     asset closing prices.\n",
        "        study_config (Dict[str, Any]): The baseline configuration dictionary.\n",
        "        param_grid (Dict[str, List[Any]]): The parameter grid for the\n",
        "                                           robustness analysis.\n",
        "        report_format (str): The desired format for the output report.\n",
        "                             Options: 'html', 'markdown', 'none'.\n",
        "                             Defaults to 'html'.\n",
        "        run_robustness_analysis (bool): Flag to enable/disable the computationally\n",
        "                                        expensive robustness analysis.\n",
        "        run_error_analysis (bool): Flag to enable/disable the computationally\n",
        "                                   expensive error analysis.\n",
        "        bootstrap_samples (int): The number of bootstrap samples for error analysis.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, Any], Optional[str]]:\n",
        "        - A master dictionary containing the complete results of all analyses.\n",
        "        - A formatted report string in the specified format, or None if\n",
        "          report_format is 'none'.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported report_format is provided.\n",
        "    \"\"\"\n",
        "    # --- Input Validation for this function's parameters ---\n",
        "    # Define the supported report formats.\n",
        "    supported_formats = ['html', 'markdown', 'none']\n",
        "    # Check if the user-provided format is valid.\n",
        "    if report_format not in supported_formats:\n",
        "        raise ValueError(f\"Unsupported report_format '{report_format}'. \"\n",
        "                         f\"Please choose from {supported_formats}.\")\n",
        "\n",
        "    # Initialize the master dictionary to hold all results.\n",
        "    master_results_bundle = {}\n",
        "    print(\"=\"*80)\n",
        "    print(\"STARTING MASTER ANALYSIS PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # --- Execute the Main Analysis Pipeline ---\n",
        "        # This is the core sequence of analysis steps.\n",
        "        main_analysis_results = run_full_analysis_pipeline(raw_price_df, study_config)\n",
        "        master_results_bundle['main_analysis'] = main_analysis_results\n",
        "\n",
        "        # Extract key intermediate results needed for other pipelines.\n",
        "        log_return_df = main_analysis_results['preprocessed_data']['log_returns']\n",
        "        validated_config = main_analysis_results['validated_config']\n",
        "\n",
        "        # --- Conditionally Execute Robustness Analysis ---\n",
        "        if run_robustness_analysis:\n",
        "            robustness_results = perform_robustness_analysis(\n",
        "                raw_price_df, study_config, param_grid\n",
        "            )\n",
        "            master_results_bundle['robustness_analysis'] = robustness_results\n",
        "        else:\n",
        "            print(\"\\nSkipping robustness analysis as per configuration.\")\n",
        "\n",
        "        # --- Conditionally Execute Error Analysis ---\n",
        "        if run_error_analysis:\n",
        "            error_analysis_results = perform_error_analysis(\n",
        "                log_return_df, validated_config, num_samples=bootstrap_samples\n",
        "            )\n",
        "            master_results_bundle['error_analysis'] = error_analysis_results\n",
        "        else:\n",
        "            print(\"\\nSkipping error analysis as per configuration.\")\n",
        "\n",
        "        # --- Conditionally Generate the Final Report ---\n",
        "        # Initialize the report string as None.\n",
        "        report_output = None\n",
        "        # Check the user's desired format and call the appropriate generator.\n",
        "        if report_format == 'html':\n",
        "            print(\"\\n--- Generating Final HTML Report ---\")\n",
        "            report_output = _generate_html_report(master_results_bundle)\n",
        "            print(\"HTML report generation complete.\")\n",
        "        elif report_format == 'markdown':\n",
        "            print(\"\\n--- Generating Final Markdown Report ---\")\n",
        "            report_output = _generate_report_string(master_results_bundle)\n",
        "            print(\"Markdown report generation complete.\")\n",
        "        else: # report_format == 'none'\n",
        "            print(\"\\nSkipping report generation as per configuration.\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MASTER ANALYSIS PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception from the pipeline, print it, and re-raise.\n",
        "        print(\"\\n\" + \"!\"*80)\n",
        "        print(f\"A CRITICAL ERROR OCCURRED IN THE MASTER PIPELINE: {e}\")\n",
        "        print(\"!\"*80)\n",
        "        # Re-raise the exception to halt execution and allow for debugging.\n",
        "        raise\n",
        "\n",
        "    # --- Return the final artifacts ---\n",
        "    # Return the comprehensive bundle and the generated report (or None).\n",
        "    return master_results_bundle, report_output\n",
        "\n"
      ],
      "metadata": {
        "id": "MHSdQNoNAQXN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}